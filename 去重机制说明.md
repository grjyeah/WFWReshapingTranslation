# 去重机制说明

## ✅ 新增功能

### 1. 智能重复检测（流式输出）

在模型生成过程中，每生成500字符就会检测一次是否出现内容重复：

- **检测原理**：检查最后100个字符是否在前面的内容中出现过
- **检测范围**：在最后500字符内查找
- **自动停止**：检测到重复立即停止生成

```python
# 示例
原文: "我们讨论了数据治理的重要性。我们讨论了数据治理的重要性..."
      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^  ← 检测到重复，停止
```

**优势**：
- ✅ 不会过早截断有价值的内容
- ✅ 模型可以充分展开说明
- ✅ 只在真正重复时才停止

---

### 2. 代码级去重（合并前）

对每个segment的输出进行去重，使用Python代码实现（不用模型）：

#### 去重逻辑

**① 句子级别去重**
- 使用`difflib.SequenceMatcher`计算句子相似度
- 相似度阈值：85%
- 如果两个句子相似度≥85%，认为是重复
- 保留较长的句子（通常更完整）

**示例**：
```python
句子1: "我们需要建立一个完整的数据治理体系。"
句子2: "我们需要建立一个完善的数据治理体系。"
相似度: 92% → 删除其中一个，保留较长的

句子1: "数据质量非常重要。"
句子2: "数据质量至关重要。"
相似度: 50% → 保留（语义不同）
```

**② 段落级别去重**
- 针对带说话人标识的段落
- 同一说话人的内容进行相似度检测
- 相似度≥85%则认为是重复
- 保留内容更丰富的段落

**示例**：
```python
[张总]: 我们需要建立数据治理平台。
[张总]: 我们需要建立完善的数据治理平台。  → 删除，保留更完整的
```

---

## 📊 处理流程

```
输入：会议逐字稿
    ↓
分块处理（1000字符/块）
    ↓
┌───────────────────┐
│  调用LLM生成      │
│  （流式输出）      │
│  ↓                │
│  实时检测重复     │  ← 每500字符检测一次
│  - 发现重复→停止  │
│  - 正常→继续      │
└───────────────────┘
    ↓
┌───────────────────┐
│  代码级去重       │  ← 使用Python代码
│  - 句子去重       │     不用模型
│  - 段落去重       │
│  - 相似度85%      │
└───────────────────┘
    ↓
检查比例（80-90%）
    ↓
输出：精简书面化文本
```

---

## 🎯 去重效果示例

### 示例1：句子重复

**输入**：
```
[主持人]：呃，那个，我们今天讨论一下数据治理的问题。
数据治理很重要。数据治理非常关键。数据治理对企业的
发展至关重要。我们必须重视数据治理工作。
```

**LLM输出（可能有重复）**：
```
主持人：我们需要讨论数据治理的重要性。数据治理对企业的
发展至关重要。数据治理是企业发展的重要基础。数据治理
能够提供有效的支持。
```

**代码去重后**：
```
主持人：我们需要讨论数据治理的重要性。数据治理对企业的
发展至关重要。数据治理能够提供有效的支持。
```
- ✅ 删除了重复表达（"数据治理是企业..."与"数据治理对企业..."）

---

### 示例2：段落重复

**输入**：
```
[张总]：我们建立了一个平台。
[张总]：这个平台很好用。
[张总]：我们建立了一个完善的平台。
[张总]：这个平台非常好用。
```

**代码去重后**：
```
张总：我们建立了一个完善的平台。
张总：这个平台非常好用。
```
- ✅ 检测到同一说话人的重复内容
- ✅ 保留了更完整的表述

---

## ⚙️ 参数调整

### 调整相似度阈值

**当前**：85%

```python
# 在 meeting_processor.py 第194行
similarity_threshold = 0.85  # 相似度阈值
```

**调整建议**：

| 阈值 | 效果 | 适用场景 |
|-----|------|---------|
| 0.90 | 更严格，只删除高度重复的 | 精确度要求高 |
| 0.85 | 默认，平衡 | **推荐** |
| 0.80 | 更宽松，删除更多重复 | 允许一定改写 |

### 调整重复检测间隔

**当前**：每500字符检测一次

```python
# 在 meeting_processor.py 第363行
repetition_check_interval = 500  # 每生成500字符检查一次重复
```

**调整建议**：

| 间隔 | 效果 | 说明 |
|-----|------|-----|
| 300 | 更频繁检测 | 可能过早停止 |
| 500 | **默认** | **推荐** |
| 1000 | 较少检测 | 可能生成更多重复 |

---

## 📈 预期输出

```
[1/49] 处理中... (输入: 794 字符, 目标: 635-715 字符) [生成中.....]
  🔄 去重: 删除 127 字符
✓ 输出: 678 字符 (85.4%)
    ✓ 理想比例

[2/49] 处理中... (输入: 801 字符, 目标: 641-721 字符) [生成中......]
  🔄 去重: 删除 89 字符
✓ 输出: 702 字符 (87.6%)
    ✓ 理想比例

[3/49] 处理中... (输入: 798 字符, 目标: 638-718 字符) [生成中....]
  🔄 检测到内容重复，自动停止
  📊 已生成 1243 字符
  🔄 去重: 删除 356 字符
✓ 输出: 887 字符 (111.2%)
    ⚠️ 警告: 输出偏多 (已去重)

============================================================
精简书面化完成统计:
  原文总长: 39045 字符
  输出总长: 33128 字符 (84.8%)
  目标比例: 80%-90%
  去重次数: 23 个segments
  ✓ 达到理想比例
============================================================
```

---

## 🔧 工作原理详解

### 1. SequenceMatcher 相似度计算

```python
from difflib import SequenceMatcher

# 计算两个字符串的相似度
s1 = "我们需要建立数据治理体系。"
s2 = "我们需要建立完善的数据治理体系。"

similarity = SequenceMatcher(None, s1, s2).ratio()
# 返回: 0.92 (92%相似度)
```

**原理**：
- 基于最长公共子序列（LCS）
- 返回0-1之间的值
- 1表示完全相同，0表示完全不同

### 2. 重复检测算法

```python
def detect_repetition(text, window_size=100):
    """检测文本是否出现重复循环"""
    if len(text) < window_size * 2:
        return False

    # 检查最后100个字符
    tail = text[-window_size:]

    # 在前面500个字符中查找
    search_range = text[-500:-window_size]

    # 如果tail在search_range中出现，说明有重复
    if tail in search_range:
        return True

    return False
```

**示例**：

```
文本: "我们讨论了数据治理。我们需要重视。我们讨论了数据治理。"
                              ^^^^^^^^^^^^^^^^^^^  最后100字符
              ^^^^^^^^^^^^^^^^^^^  在前面找到了相同的

→ 检测到重复，停止生成
```

---

## ✅ 优势总结

| 特性 | 之前 | 现在 |
|-----|------|------|
| **停止条件** | 固定15000字符 | 智能检测重复 |
| **去重方式** | 截断 | 代码级去重 |
| **去重精度** | 低（简单截断） | 高（相似度85%） |
| **信息保留** | 可能丢失 | 更完整 |
| **幻觉风险** | 无（模型） | 无（代码） |
| **正常比例** | 75-85% | **80-90%** |

---

## 🧪 测试建议

### 测试去重效果

```bash
# 运行测试脚本
python test_sample.py
```

查看输出中是否有：
- 🔄 去重提示
- 删除字符数量
- 最终比例是否在80-90%

### 手动验证

1. 检查 `processed_chinese.txt`
2. 查找重复的段落
3. 确认去重是否正确

---

## ❓ 常见问题

### Q1: 去重会不会删除重要内容？
A: 不会。去重逻辑基于85%相似度，只删除高度重复的内容。如果语义不同，即使用词相似也会保留。

### Q2: 为什么有些segment输出>90%？
A: 可能模型生成了大量有价值的扩展内容，去重后仍然偏多。这是正常的，只要不超过100%即可。

### Q3: 去重会影响处理速度吗？
A: 几乎没有影响。去重使用纯Python代码，速度非常快（毫秒级）。

### Q4: 如何关闭去重？
A: 注释掉第480行的`result = self.remove_duplicates(result)`即可。

---

**版本**: 3.0
**更新日期**: 2025-01-XX
**新增功能**: 智能重复检测 + 代码级去重

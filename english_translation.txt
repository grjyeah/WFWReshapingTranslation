[Yu Chang]: Can you hear us speaking?
[Host]: I will first check the network and voice. Please confirm when you can hear me.
[Yu Chang]: Okay, good.
[Host]: It is four o'clock in the afternoon now. Our data governance tool exchange meeting for this afternoon is about to start. Thank you very much for your participation and support from all of our leaders and colleagues present today.
[Host]: The main content of this conference will be an exchange on issues related to data management with Enhe Beijing Information Technology Co., Ltd.. We will discuss both business-wise as well as technical aspects, and look at some problems encountered in daily work along with their solutions.
[Host]: Next, please allow Mr. Yao Jun from the Technical Director of Enhe to present our company's basic information and its achievements and case studies related to data governance within this field.
[Yao Jun/Technical Director]: Good afternoon everyone! I am Yao Jun from Enhe Company Limited. Today we will discuss issues regarding data management and asset administration together.
[Yao Jun]: The discussion today is divided into three parts:

First, the development history of our company: Since 2012, we have been focused on the field of data governance; by 2020, this was further extended to include work in managing assets. We now hold extensive experience and expertise within the industry, especially with a focus on financial institutions.
[Yao Jun]: Our main clients are large state-owned commercial banks such as the Bank of Communications, non-bank financial institutions like Everbright Securities Co., Ltd. and Minsheng Securities Co., Ltd.; we also have wide-ranging applications in other industries including Pacific Insurance Group and Taikang Life Insurance Company Limited. These cases all demonstrate our professional capabilities within this field.
[Yao Jun]: Over the past three years, we successfully implemented a series of important projects such as building the data governance platform for Bank of Communications along with related consulting services; system renovations for multiple urban credit cooperatives including shareholding commercial banks and significant city-based commercial banks like Nanjing Rural Commercial Bank and Jiangsu Provincial Credit Cooperatives. These achievements indicate our continuous innovation capabilities within this field, alongside its practical application value.
[Yao Jun]: In terms of team strength, we have a highly professional group of more than 200 data governance engineers with extensive experience in the industry. Our reputation is well-established among peers through ongoing efforts and improvements to our technical systems that ensure high-quality service for clients.

[Host]: We also come from a state-owned background; our company's development status has been very good.
[Zhang Manager]: Next, I will introduce our enterprise-level data asset management platform along with its core functions. This includes:

Firstly is the full-process management of assets throughout their lifecycle; secondly we have relatively advanced overall capabilities in terms of governance which also include a wide range of AI functions covering from inventory to registration processes for all stages of an item's life cycle. Additionally, we provide a fully domestically produced data modeling tool that supports custom development and has seamless integration with simple models and ERDs (Entity-Relationship Diagrams). We manage large-scale computational tasks on numerical data as well as the entire lifecycle management of various forms of data.
[Li Manager]: I agree with Mr Zhang's point. Our platform can effectively support overall business operations.
[Host]: Let me briefly introduce our asset management platform in relation to other relevant platforms: From an assets perspective, we are involved in modules such as original standards and main data requirements within the Data Governance layer of the system; these connect directly back to source data warehouses or core systems from various layers. These raw datasets will then enter into a middleware for further administration before being elevated up through multiple levels including management and utilization of asset information itself. The overall architecture is clear: it starts with infrastructure, moves onto services access level, finally reaching the application layer; functionality covers standards, security measures as well as asset management.
[Zhang Manager]: From an operational point-of-view, this platform primarily involves three layers - data governance (including standardization), lower-level data security and upper-layer applications that connect directly to our service interfaces. Furthermore, model tools can also be used in the context of data governance; let me interrupt for a moment:

As the business director I will interpret why we are discussing an asset management platform today: Earlier together with all presenters here we reviewed some case studies - if you're too close it might sound muffled. So just to reiterate, within banking sectors where our solutions apply there is only two specific scenarios - one being a data control system for the IT department, Data Architecture and Management Departments; another scenario would be around an asset management platform with its specific use cases. The needs mentioned in this meeting are mainly related to what we offer here today. This diagram shows how typical governance tools fit into our overall framework.

[Host]: Good afternoon everyone! Today's conference will focus on issues regarding data governance and exchange of ideas among all presenters.

[Zhong, General Manager]:
We need to establish a comprehensive data governance framework. Data governance is extremely important and critical for the digital transformation of enterprises.
[Li, Manager]: I agree with Mr. Zhong's insights. Our platform can provide effective support.
[Wang, Ph.D.]: There are seven modules in our middle layer. Those that may be relevant to this meeting include standards quality, raw data plus data models, and data requirements. These along with other models sit approximately four or five levels above the bottom which includes data security and classification/grading needs. The document from last year's end mentioning these five tools is currently a primary part of our middle layer that can meet your existing demands.
[Zhong, General Manager]: Whether it’s now being manually managed or regulated by hand through this platform, we are able to integrate the current requirements with those tools. We launched our data asset management platform because in Shanghai Pudong Bank over the past decade its data assets have been at least six point zero up to seven point zero and has been running for ten years.
[Li, Manager]: This asset platform provides us a ready-made solution. May I ask if the current sound is normal? Please move closer to your microphone. I will directly address this issue regarding it.
[Host]: Can you hear us speaking?
[Wang, Ph.D.]: Indeed we can hear your voice; may you confirm that please?
[Li, Manager]: Yes, I can clearly hear what you are saying. This asset platform is actually one of the core scenarios in our meeting today. The emphasis here was on data asset management and inventory through this tool to realize value.
[Zhong, General Manager]: In this process, on our right side there’s a tool for building models. Modeling tools have two main purposes: One is to play an active role when setting up and managing existing controls; the other being that during your design of data architecture it can be organized neatly. This is a crucial part throughout the entire data governance process.
[Wang, Ph.D.]: The foundation platform mentioned earlier, if those four hundred core datasets along with CR (customer relationship) plus any related integrations are put together, this tool serves as an intermediary in our middle layer. I previously stated that through asset platforms we can check and understand how assets are being used and achieve a seamless integrated process from the source of data management to its final consumption within our departments.
[Li, Manager]: So, how is this data governance platform actually implemented? How will my specific data assets, classification/grading, as well as regulatory compliance requirements be represented?
[Zhong, General Manager]: This diagram here today's core discussion. Any questions we can discuss and exchange in detail later on.
[Wang, Ph.D.]: Excuse me for asking this; it seems that the part of our chart dealing with data asset management and application looks quite similar to some boxes under the section about data security below. Could you please provide a brief explanation?
[Me]: Let’s now interpret what we see in terms of data safety issues. For example, during classification and grading:
[Me]: This is a way to address problems related to our assets.
[Me]: From a business perspective, it's one aspect considered from the technical practice for project initiation. Through this process, we can understand how these asset data are applied and see where they're being used specifically.
[Me]: So please look at this chart. That means each module here represents an actual component part of our system. If you align those seven modules with above's mentioned seven functions:
[Me]: In fact, it describes what the functionalities in terms of data safety entail? For example, classification and grading are parts of them.
[Me]: Such as static desensitization (such as encryption for sensitive information) plus dynamic desensitization. Additionally, there’s audit functionality within our security aspects:
[Me]: It ensures all operations comply with regulatory requirements by recording it to be reviewed later on. However, the mentioned seven modules are just from a functional point of view.
[Me]: Actually what should have been placed here is classification/grading, desensitization (both static and dynamic), as well as related security and audit compliance – this could be an area where our previous settings may have led to some misunderstanding:
[Me]: Therefore, let me re-explain these points: Classification/grading, data desensitization including both static and dynamic forms plus the relevant aspects of security and audit compliance are key modules for ensuring data safety.
[Me]: Got it?

Sure, here is the translation of your meeting minutes:

---

Next, we will continue with the discussion on the next section. From a technical perspective, we are currently using micsql and ES to handle data persistence issues while adopting a microservices architecture to support front-end development work.

[Manager Li]:
Through the application of these standardized processes and intelligent processing technologies, we can ensure that data collection, development, and management are more efficient and accurate. This will also meet enterprise-level needs.

[Host]: 
We can manage multiple systems as well. Since our company might have several sets of standards to address different regulatory requirements, it could mean having multiple different standard frameworks.
[Manager Zhang]:
Within our platform, we can establish two or even more standard frameworks. For the analysis capabilities from冠标 (standardized) and公标 (publicized), there is a comprehensive comparative analysis overall; for report generation in the public domain section, everything is quite clear. We have a set of financial industry standards vocabulary that will be frequently used by our clients.

[Manager Li]:
Setting up these frameworks would be relatively fast, we can also create them online to ensure the freshness of each standard.
[Manager Zhang]: 
Regarding models, there are two main categories: micro-end and B-end. The latter mainly serves as a model management function; in terms of supporting reverse modeling and forward modeling within efficiency slightly lagging behind client-side operations.

Overall speaking, from creating new models whether it's through forward or backward modeling up to the subsequent review and release process can all be achieved via our system either by project dimension or system dimension.
[Manager Li]: 
Our intelligent features along with standards provide smart recommendations. After building a model, we could then proceed with benchmarking; this is part of AI functionality which will have more detailed explanations later on in another meeting. Generated DDR statements after modeling can also be matched against original data to analyze the consistency between our models and the original datasets.

[Manager Zhang]: 
We have an end-user support tool that directly references already used standards for forward or reverse model building; then, these models are aligned with existing standards post-building, generating DDR statements within the platform which can run scripts.
[Manager Li]:
Through these functionalities, we can achieve precise analysis and efficient management of original data throughout this entire process. The standardization and intelligence ensured during this process would enhance overall efficiency in model management and data analytics.

[Manager Zhang]: 
This way, it will help to avoid some small errors when building models later on.
After the matching with standards:
Ensuring that a model meets standardized requirements is achieved; the whole lifecycle of model management covers creation, review, and publishing including project management and configuration tools support.
[Engineer Wang]:
Through our platform we can complete all configurations and administrative tasks within the system. Modeling tools are rich in visual features, enabling efficient intelligent annotation.

[Zhang Manager]: 
Next, please briefly introduce us to the main data aspect with a focus on quality control issues. Overall, an annual quality plan is formulated which gets broken down into departmental solutions generating business rules implemented through technological means; this approach works from top-down.
[Manager Li]:
Another method involves starting specifically at problem points: when we find data issues in production or test environments, improvement measures are proposed based on these problems and relevant metrics scored via the platform's reports. These actions can be categorized by department and rule.

[Zhang Manager]: 
Could you please confirm if there is a tracking module for quality management? If this system automatically assigns tasks to responsible personnel?
[Manager Li]:
We have an issue ticketing system where from problem initiation through resolution, everything happens within the platform thus ensuring data loopback management.
[Host]:
Today's discussion revolves around resolving issues with data. The closed-loop process can be managed in our platform.

[Treasurer Zhang]: 
Evaluation is straightforward; we provide effective support via this platform.
[Manager Li]:
May I ask if Data Quality is a standalone system? It’s a standalone module within the platform called 'Data Quality'.
[Team Lead Wang]: 
Where are its primary use cases?
[Host]:
For example, in our group's system integration processes where control over data from System A to System B might be required. This is one way we manage Data Quality.

Manager Li:

[General Manager Zhang]:
So, could you please explain the specific use cases for this data quality module? If there are no particular scenarios in mind, I will briefly outline some proactive approaches to improving it.

[Manager Wang]:
In our regulatory reporting system, we have a plan for enhancing the accuracy of customer information. This includes entering an overall improvement plan into the platform. For instance, one goal could be to increase issue resolution rates above 80% this year and allocate tasks accordingly among various departments.

[General Manager Li]:
For example, after administrative plans are distributed to bank branches or corporate clients, they might generate corresponding rules based on their business needs. Personal customers would need checks for nine key elements while companies may only require three to five depending on the context. After defining these operational policies, technical personnel can then implement and enforce specific validation criteria.

[General Manager Zhang]:
Can we configure those rules so that they also support real-time scenarios like data transfer from System A to System B?

[Manager Wang]:
Yes, this is possible within our regulatory reporting system. For instance, when handling personal customer information, an improvement plan could be entered into the platform and assigned tasks for execution by relevant departments.

[Host]: This approach not only ensures that quality standards are met but also enhances business efficiency and accuracy.

[I]:
I am likely to move on soon because from my perspective as a platform user, it mainly revolves around what this teacher mentioned – integrating with data platforms.
[Teacher]:
If your source is 400 miles of data, I do not know about the other side’s data platform.
[I]: If using Rouderer's Colrodirea, you might have CDC (Change Data Capture) to replenish t+1 from t. In our quality management process across any platforms we use:
[Teacher]:
Right?
[I]: Regarding your scenarios for regulatory compliance – Version 2.0 is what I mean here.
[Teacher]:
Then, in terms of governance and construction, it should be seen as a channel between your platform and the application to regulatory data within this entire data hub.

[I]: In real-time t+0 stage with flick CDC integration? You run out first before integrating with flick CDC. I'm just responsible for extracting business rules and processes from you.
[Teacher]:
That is, these rules need to be applied into corresponding systems?
[I]:
Then the system can handle related quality issues of data – correct?
[Teacher]: That's what it means?
[I]: If moving forward to Fliess, then I'll read your words. Thank you for that knowledge.
[Teacher]: Any other questions on this matter?

[I]: No more; let me continue with my presentation.

[Teacher]:
Briefly regarding assessment: We may evaluate departments or meetings based on issues discovered during the data governance process.
[I]:
This can be handled within our overall scoring model under an assessment module. 
[Teacher]:
Next, we move to discussing assets in general terms...
[I]: From a business operations perspective, it's mainly about showing asset distribution and directories through portals like the Asset Map.

[Teacher]:
For personal management centers – managing interactions with individual users on their assets.
[I]:
Then for overall operational aspects of our platform including dynamic stats are handled here. Next is an important focus area: inventory checks.
[Teacher]: That process from resource to asset?
[I]: First, we start by doing a stock check and authorization.

[Host]: For the inventory work in our system can be automated – completing some information updates through this system.
[General Manager Zhang]:
Additionally, within the platform, security classification and tagging are also completed. Business tags are applied during this process as well.
[Manager Li]:
In asset lifecycle management from stock check to release for review down to final usage certification is managed uniformly in our platform.

[Host]:
We have already discussed all the issues related to data governance, including original raw data and previously collected datasets. This also includes quality rules and standard models which will be synchronized into assets.

[Zhang Manager]: Through different types of metadata, we can supplement content in the asset records fully visible within them. The main discussion revolves around the value of data governance tools and platforms.
[Li Manager]: Currently, we have a tool for intelligent lineage that works well across various industries. This tool enables rapid extraction of scripts, original raw datasets from databases, and displays these clearly.

[Zhang Manager]: For our technology department, this is very useful in performing impact analysis among other operations; it's convenient and intuitive. The simple meta-architecture display aligns with the asset management platform while its underlying tech architecture uses a microservices structure.
[Host]: Function-wise, overall support for mainstream SQL statement parsing ensures an accuracy rate of over ninety-five percent or even higher wherever there is code and circle. This can be achieved in both places.

[Zhang Manager]: Unless data are abnormally unstructured, minor adjustments will suffice to resolve issues. We also parse operator-level syntax and build a full-chain enterprise-level data source from the origin system through warehouse application layers.
[Li Manager]: By this means, we can achieve comprehensive coverage and management of entire processes.

[Host]: This concludes our product introduction; next is an overview of future capabilities in terms of data governance. 
[Zhang Manager]: In terms of specific measures and methodologies for data governance:
At the standard level, a complete financial industry standards library will be provided to allow companies to choose from among them. The set includes basic data standards management with enterprise-level metadata dictionaries which are widely used by banks; we also integrate insurance-industry common data dictionaries.
[Li Manager]: For managing foundational standards, our current tools include an enterprise-wide dictionary that is commonly applied in the financial industry and other sectors as well – these have wide usage rates and acceptance. Zhang: In constructing a comprehensive standard library using a closed-loop management approach—coordination between data models, original datasets, and data standards to achieve standardized management.
Unlike previous methods where we started from collecting raw data for dictionaries then post-control and comparison; now we introduce the standard during new model construction ensuring that both forward or reverse modeling generates tables in compliance with norms directly.

[Zhang Manager]: If using forward modeling, after completing a model, our system automatically generates database schema. This guarantees consistency and standardized levels of all datasets.
On the other hand, if using backward modeling would require first aligning empty structures to improve standardization by optimizing original architectures.
[Li Manager]: In new systems, comprehensive alignment work is usually required including but not limited to matching Chinese field types with existing standards perfectly; for legacy systems during renovation we recommend adopting a one-by-one mapping approach – that is, converting all non-standard fields into those in compliance with the data dictionary and completing corresponding transformations.

[Zhang Manager]: This concludes our specific measures and methodologies for improving enterprises’ digital transformation capabilities. By these means, we can effectively save certain content within platforms to know which field maps to what standard; this allows clear matching of them while resolving some cases of homonymy but different meanings.
From the perspective of original data, our overall approach is as follows:

An original dataset knowledge base exists that includes collected datasets and raw data from various databases. These are imported into our platform for comprehensive management involving full-text search, difference analysis, version control, lineage tracking etc., all based on our data governance system advocating a model-based abstraction mode of management.
During design phases we start referencing standards in modeling; post-generation DR statements which get executed directly to the database ensuring dataset quality. For existing systems they can be imported and optimized into platforms by reverse engineering during operations as well where overall lineage analysis is possible through our platform.

Ultimately, all of this is tied to the overall data application—content integrated into our platform from design, development, and testing phases. From a quality standpoint, promoting a closed-loop management approach involves prevention before events occur, monitoring during processes, and improvement after issues arise.

[Host]: Let's discuss how we can enhance overall data quality. Common data quality management control models typically involve three approaches:

[Zhang General Manager]:
The first is the annual planning model. This could be an enterprise-level plan where a comprehensive data quality enhancement scheme is formulated and then broken down into various departments. Each department proposes solutions tailored to their specific areas, develops business rules, and sets inspection criteria. Regular assessments are conducted.

[Li Manager]: The second approach involves specialized governance initiatives. For instance, in scenarios such as regulatory reporting or predictive analytics where targeted improvements need to be made, the platform is used to implement standardized processes for achieving these objectives.
[Zhang General Manager]:
The third method focuses on problem-solving from an issue-based perspective. We first identify specific issues and delve into their root causes and impact scope. Through incremental solutions of individual problems, we gradually improve data quality across all dimensions while also implementing management measures like process improvements within the platform.

[Host]: Next, let's discuss how work is carried out in terms of classification and grading for security purposes. The current standard practice follows the "Zero Yamoqi" guideline issued by the People’s Bank of China (PBOC). Additionally, there are national standards on GPT safety categories that were released in 2024. These guidelines form an integral part of our platform.

[Zhang General Manager]: In terms of data security management, we primarily analyze and categorize from two perspectives: first, ensuring all operations comply with predefined security standards during the modeling phase by clearly labeling fields; secondly, using specialized tools to directly mark incoming raw data at its source for secure classification.
[Li Manager]: Looking forward, it is recommended that more standardized methods be adopted to achieve efficient data security management. Such approaches have been widely used in the industry and are considered an excellent solution.

[Host]: From a perspective of original data, this work involves significant effort. Ensuring all original data meets regulatory reporting requirements when dealing with future compliance demands will also require us to ensure that all our raw datasets meet these standards.
[Zhang General Manager]: Looking forward, it is recommended prioritizing the standardization layer in building an even more robust governance structure. This not only enhances overall data quality but also supports and improves our standardized efforts.
[Li Manager]: I agree with Zhang's insights. Especially for security classification analysis, we can label various original datasets against standards based on different security levels, implementing encryption and access control measures accordingly. De-identification tools could be used in specific operations too.

[Host]: Let’s discuss the issues of data resources and assets next. This area has become a hot topic recently. From our perspective, streaming data is an important component of the fifth productivity. Within our platform, we can achieve effective organization and management processes—standardized and normalized handling forming unified resource libraries that gradually transform into usable asset databases.
[Zhang General Manager]: In early stages, there's first work on inventorying and validating ownership; this includes assessing value to ultimately confirm its归属关系。
We have accumulated rich practical experience which provides corresponding templates for reference. As part of the guiding scheme, by leading companies completing the overall process flow followed with review optimization before official release.
[Li Manager]: In actual operations, from determining scope through clarifying content and developing specific templates are critical steps. For instance, when detailing public financial channels or customer insurance products we can clearly outline each project's specifics to lay a foundation for subsequent analysis.

[Host]: This is an example of simple directory related to the insurance domain; please refer to it. In this way, we will better understand and apply these concepts.
Other examples done by other insurers should be associated with east. For our part after building up asset directories,
there would be secondary and tertiary levels. The third level links directly back to the Zero Yamoqi document for matching purposes.

Security levels are automatically assigned to our directory. Finally, standards or raw data will be mounted onto the sub-directory catalog after classification is complete; this naturally results in a level of categorization. Here’s an example of how it works for asset directories. Currently, table entry is also a popular topic: both simple and complex entries require collaboration among various parties such as consulting firms, law firms, finance departments, and subsequent exchanges.

Regarding the application of AI:
Standard-quality assets have support from AI functions; they can automatically generate data labels and complete standardization work in an intelligent manner. The auditing process itself is also conducted through intelligent means. Script generation similarly can be done via AI tools that are automated by reading information from existing rules to produce high-quality operations related to the task at hand. Asset management includes smart inventory checks and classifications; all of this relies on platform-based intelligent tools.

Intelligent security classification and grading are common scenarios, so I won't go into detail about them now.
In terms of standards:
Newly added standards can be automatically generated by our platforms; referenced standards can also be done in an automated manner. For quality enhancement purposes, we leverage high-quality data assets’ quality libraries and rule sets to generate corresponding rules based on usage scenarios. Through these measures, overall data quality management is improved; this was the general overview provided by the vendor.

Next, please feel free to ask questions or voice other concerns.
[Vendor Representative]:
There's another aspect: vendors will provide a simple demonstration of tool interfaces and functionalities for everyone’s review. Is there any question anyone has?
[Host]: I can't hear anything now; could you confirm if you can hear me? If so, please respond.
[Warden]: Hearing clearly, just taking screenshots at the moment.
[Host]: Okay. Please take a look at what the vendor representative presented about this tool platform and see whether there are any questions or concerns regarding their daily work that need clarification.

[Tong General Manager]:
When referring to intelligent classification and grading capabilities, did you use large models?
[Vendor Representative]:
Indeed, we have used large models in our intelligent classification and grading process.
[Tong General Manager]: Could you please tell us where your model is deployed? Is it a custom one integrated into this tool or the vendor's own?

Vendor representative: Yes. Our models are integrated within these tools as small specialized models with enhanced functionality.

[Host]: What type of model specifically, for example in terms of parameters?
[Vendor Representative]:
We train on common knowledge bases and data dictionaries to deploy them across our systems for recommendation functionalities; this includes some large-model components.
[Tong General Manager]: Okay. Regarding the 400 issue: how did you previously interface with the Bank of Communications (BOC)?
Vendor representative: Our previous interfacing was primarily done through their EGIT platform into IT systems, which involved data extraction and transmission. For later adaptation, we may opt for more suitable methods.
[Tong General Manager]: If custom solutions are required specifically, what form would you use? Would it be via the EGIT or another method?
Vendor representative: We used to interface with BOC through their EGIT platform; specific adaptive measures will depend on particular circumstances.

[Tong General Manager]: Good. Could we ask if any of our business teachers have questions regarding the current data governance solution? If so, please elaborate briefly.
[Vendor Representative]:
We can log in from the homepage and showcase interfaces and functionalities to everyone for a better understanding of how this tool works and its applications.

[Host]: All modules are visible on the home page; it’s an integrated management process. Data assets, quality assessment, and retrieval data are all accessible through our interface.
[Zhang Manager]:
My point is that we can simply check some content via these interfaces. On the standard section of the homepage, one can view comprehensive standards including reference statistics for various standards as well as their respective citation counts.

[Li General Manager]: From a perspective of managing different systems' standards effectively—such as in bank systems—one should be able to display each specific standard and review detailed referencing information.
[Wang Manager]:
In terms of personal information, we can also showcase this here. By examining the standards section, one could maintain basic information, business attributes, technical details related management tasks.

However, I have a question: Is it possible for every item’s associated business relevance, business rules, and technological management to be directly displayed on this page? Or must users click through to access detailed content?

[Host]:
The detailed information is stored in the internal configuration and can be edited, saved, and then displayed on a list. This represents the default configuration mode. If the content is too long or if more detail needs to be viewed, other methods are available.

[Zhang Manager]: For standard audits, our audit process will be shown here along with published versions for reference. Specific information about each version can also be seen, such as documentation of technical and foundational standards being issued in this interface.

[Li President]: Regarding the analysis of standard references, we use selection to view specific situations. Since data may not always be comprehensive, a brief introduction to our standard system and its applications will follow here.

[Host]: You can see which standards or models reference what on this side.
[Standards Expert]: All these standards are displayed right here. For example, for difference analysis:
We have the current version of one standard in front of another within a standard system's framework. The differences can be compared over there, and checking against referenced standards is our way to check which standards we're using. By looking at their referencing rate, or how often they are used.

[Host]: For example, if some system references our standards with its own referencing score, that information will also display here along with other relevant data. Regarding the lexicon:
We maintain a dedicated lexicon for this purpose, managing words and English phrases separately. Each word has an individual management area as does each code value.

[Standards Expert]: Configuration management allows us to set up standard systems where any configuration items can be configured from here. For example, the display of information about standards mentioned earlier are maintained in this interface. I also want to ask a question: In your standard management process, if you have some field or data item,
is it possible for that system screenshot image to be attached?

[Host]: Can images be attached? Well, from here on out we manage the usage and storage of standards through models and original data blocks. Yes, I understand; sometimes in previous scenarios like when creating ST-related data standard definitions, screenshots might have been taken from authoritative systems. However, currently this is managed differently.

Can you attach a screenshot image to that field or item? For example, if future system images are available,
can we directly link them here?

[Standards Expert]: Anything can be added as long as it's personalized and the type allows for an image attachment. Yes, all fields in our system are configurable; any new fields you want to add can.

[Host]: Okay, that sounds good. Now let’s have a look at whether there is anything else from business teachers or IT experts regarding this module? If not,
we will move on to the next one: looking into standard quality metadata for models and original data.
The model isn’t concerned with standards reference; it only deals with basic original data.

[Host]: Regarding the model of standard quality, we can't look at that right now because there's no time. So I'll focus on original data as they are linked to our models. 

For standard references in models:
We will just do a general overview for now.
Currently, through system forms and entries,
we display various systems' original data here on the right side. Regarding collection tasks:

Each task is an independent unit; we can create new comprehensive tasks by collecting from one or more systems like A followed by B.

In terms of jobs:
We allow multiple tasks to be associated with a single job, for instance creating sequential jobs that match our defined tasks. We also configure schedules here, including the frequency and timing of execution—whether it runs every minute or is set on a timer.

All subsequent logs from these jobs can then be viewed in this platform.
That’s all we have time to cover today; thank you for your attention!

We can directly see the results we have generated,
and here is where you would find all of our logs. By opening it, one could view an overall log and execute operations from there.

You are also able to see the total duration for jobs. For configuration purposes,
we can set up how systems along with metadata will be configured.
Mount points that need to be mapped in terms of lineage relationships:
What we wish to capture? We configure this here. Original data versions:
We compare differences between original datasets within a single system.

These comparisons are done from version-to-version, and even comparing production against testing or multiple production versions at once for development and testing purposes.
Version types could include both development versus test as well as production versus test,
all of which can be executed here. The management of our source data primarily revolves around capturing and displaying information;
however, the relationship between original source data and classification levels needs to be reviewed.

Let’s see how we should look at this:
The way in which asset classes are managed within assets is through common themes or metrics.
Security classifications have not been included yet as part of our project scope for now.
[Host]: Let us discuss the management of these data assets. By using classification and grading methods, it can be better to manage them more effectively.

[Zhang Manager]: We need a robust governance system in place for all this information. Data governance is crucial for digital transformation efforts within an enterprise.
[Li Manager]: I agree with Zhang's views; our platform will provide effective support towards that end.
[Engineer Wang]: On the current platform, business classification of assets happens through commonly used themes and metrics systems. Security classifications are not currently part of this project scope.

[Host]: How do we conduct an inventory check?
[Li Manager]: We can directly configure a task to complete it; this is also done in an automated fashion.
The knowledge base will help us achieve the goal, while logs related to these checks would be recorded as well.
[Zhang Manager]: In terms of operations, there needs to be attention given towards dynamic reports. For instance,
we could monitor metrics like clicks and comments real-time through our platform.

[Engineer Wang]: By analyzing how users access assets, we can understand the scale of overall asset usage along with specific details about their use.
If errors are detected or corrections need to be made on any part, these changes will be implemented directly within this system as well.
[Li Manager]: When capturing source data in our management module,
will further operations such as classification and grading also happen here?
[Zhang Manager]: Indeed; there is a close relationship between the assets we capture and our standards. These are viewed as another form of asset themselves.

[Engineer Wang]: Actually, your data standards should be directly linked to original source datasets.
Moreover, this connectivity needs to extend across all three components: 
data assets, metadata (standards), and raw data in design terms.
[Host]: So it is fair to say that during the whole process,
all standards, models, quality metrics are being synchronized with our asset management module. The ultimate goal would be a unified statistical function?
[Zhang Manager]: That's correct; within practical applications, whether for modeling or other operations,
this system will perform tasks accordingly.

[Host]: We can reference data standards even when reverse-engineering the model during original dataset creation.
[Zhang Manager]: A robust governance structure is crucial to digital transformation efforts of an enterprise. This cannot be overstated enough.
[Li Manager]: I agree with Zhang's views; our platform provides effective support towards this goal.

[Host]: First, on quality front,
we can see a plan for improvement and quantity statistics at the homepage. From that perspective we could make plans to manage data overall while clicking through details of solutions in case studies such as layer one regulation submissions.
[Zhang Manager]: After establishing these standards, they would be rolled out so relevant departments generate specific implementation strategies.
For instance, our Data Quality Team will handle comprehensive solutions for reaching the goal. What choices do I need to make if that's what we're trying to achieve?

For example, the procedures for business rules and monitoring template creation.
[Manager Li]: These processes are designed by the Technology Department to establish rules and monitor their implementation. For instance, when addressing customer issues from a top-down perspective, the Business Department first proposes solutions and checks specific factors of individual customers to enhance particular elements' quality. This is achieved through corresponding business rule establishment.

[Host]: After completing these operations, the Technology Department will feed back results for further confirmation and adjustment by relevant departments. We hope this process ensures consistency between data quality and business needs to provide a solid foundation for our company's digital transformation.

[Host]: The circle configurations are set up here, including how problem statistics should be counted, both in terms of partial and overall datasets.
[General Manager Zhang]: From this interface, we can also see which solutions are associated with specific business rules. For example, new rule creation or referencing existing deployed ones is possible.
[Manager Li]: Indeed, it's a top-down improvement process starting from issue tickets to address problems. After identifying that customer IDs were empty as an issue, I initiated subsequent checks through this problem ticket.
[Host]: I found instances of missing customer IDs in the city and instructed relevant Technology Departments to investigate the causes and formulate solutions accordingly. In terms of data quality enhancement, we need to develop specific solutions and ensure these business rules are implemented into final inspection rules.

[General Manager Zhang]: These association relationships and interconnections can be seen within problem ticket details here as well. All configurations are adjustable.
[Manager Li]: Let’s look at a particular inspection rule then. In the Technology section, custom development of circle check rules is possible with logs visible here. What will the final data report look like?
[Host]: First, let's clarify how we handle problematic datasets. After running circles, it shows all information about issues and de-identification settings.
[General Manager Zhang]: During problem resolution processes, manual handling or direct execution in production environments might be required with follow-up actions taken next day.

Welcome everyone. We will discuss the quality reports shortly. Our quality reports are based on fixed templates covering various aspects of data governance.
[General Manager Zhang]: Establishing a robust Data Governance framework is crucial for our company's digital transformation, which we must prioritize greatly.
[Manager Li]: I agree with General Manager Zhang’s insights. Our platform provides effective support; within the five-year plan, specific validation and business rules are included as part of fixed templates that can be imported into basic ones.

In summary, data governance focuses on enhancement plans, solutions, problem statistics, work tickets, etc., which all appear in reports despite incomplete current datasets.
[General Manager Zhang]: We previously established some data standards such as policy topics linked to customer premium payments. Under each topic are numerous data items with many quality rules configured for them.

[Manager Li]: Ultimately, I want a comprehensive view of the entire domain's dataset issues – specifically, how many problems were found in certain types and their resolution rates.
[General Manager Zhang]: Actually, we can achieve this by creating different categories. For example, reviewing each topic one year later to see which data items have been identified with quality issues and what changes occurred regarding these datasets’ quality; statistics on total number of discovered quality issues, resolved ones etc., are available.

Can you view such statistical reports? The data quality report I showed earlier seemed to still differ in terms of problem dataset counts. Specifically, within the problematic dataset section, it shows how many problems were caused by specific systems or institutions and their resolution rates.
[General Manager Zhang]: These can be configured through tools; changing display topic names is an example. This means we can modify these settings as needed throughout our usage – just like adding new functionalities flexibly to meet varying demands and situations.

[Host]:
We have a default configuration. You can add other content directly on the page.

[Giovanna]: Understood, thank you. The quality rules section in this portal is configured right here on the page; just adding fields will suffice.
[Li Manager]: Yes, these are some system configurations that we can distribute data governance mechanisms through. We can confirm actions when writing and setting up those rules. For example, you could issue a rule based on an organization or via field mapping for distribution during this process.

[Host]: We also have the capability to automate this distribution. That's all regarding quality issues here; please let us know if there are any other questions.
[Giovanna]: Let’s ask our esteemed teachers – do you see anything wrong over here? I, Giovanna, would like to inquire: in previous collaborations with different companies, did we ever encounter similar problems?
[Li Manager]: Yes, as you mentioned — when a data quality issue is detected and needs to be recorded on the platform along with creating an incident ticket for follow-up. Reports are formed afterward.
[Host]: Now I want to ask if such issues were commonly duplicated in different places? For instance, at our company, we first address awareness-level problems through one-time platforms following process protocols; IT personnel record and solve these within the same system while also re-registering them on this platform for follow-up. Essentially, a single task is repeated across three separate locations.
[Giovanna]: Have you ever encountered similar issues as other companies faced during project implementation?
[Li Manager]: Regarding integration in our requirements with other companies, we can create many data-related needs and quality standards here; by connecting the work order system to this platform, relevant information will be pushed forward for display. This facilitates subsequent operations.
[Host]: I'll add onto that — within distribution mechanisms, we have flexible options like organization or field mapping while supporting automated processing flows; furthermore, in terms of data issues, unified recording and tracking can prevent duplicate registrations.

[Host]: We use a Data Requirements Platform template to address this issue. It's about solving problems at their source. During the PPT presentation earlier, we realized that version 2.0 focuses on post-event data quality management as your data quality could occur either in platform or application phases.
[Li Manager]: What I'm saying is that you may encounter these issues from an applications perspective; regarding Data Management, I don't know how our original control platforms managed this before — there was no effective governance over the Data Warehouse and OITP 400 via data platforms. As mentioned by your esteemed teacher earlier, three different perspectives might be needed to address those points.
[Host]: We need a unified approach for managing these from a requirements perspective; that's what I mean when I say "shielding" your concerns. Implementing version 2.0 in our Data Platform can be managed through one centralized data quality management platform, and it’s something the banking insurance industry is currently facing. Just as mentioned earlier, this data-driven approach to building such a quality platform is how we do things.
[Host]: When doing data governance while constructing assets, it's very important! As mentioned before, that was about pre- and post-event monitoring of our Data Quality Platform; most companies only start focusing on these issues in the aftermath. Our current Quality Platform aims at solving this from beginning to end.

[Giovanna]: Thank you for your understanding. Regarding data quality management, please have other business teachers such as Rose or Clean check if there are similar issues within their departments and also ask Mr. Hao Chuan if he notices any problems; if not, let's focus on the remaining functionality modules while paying particular attention to models.
[Li Manager]: I just mentioned about the connection between our discussed model and standards in this platform for achieving standardized operations. To design a model, we need an identified data entity with its attribute name; then standard recommendations come into play when selecting fields based on already established standards leading to intelligent suggestions.

[Li Manager]: Through this method, these field mappings can align seamlessly with existing standards while ensuring compliance with predefined application rules.

[Host]:
It is like 100%. It directly recommends that field to you. If there are multiple types, then possibly several uses will come up and the data items related to you would be recommended more often. I didn't quite understand what you were saying earlier; could you clarify if this module is a Data Model Module? Yes, yes, it is. The simplest scenario is that various ADs might perform data modeling based on either end devices or client-based models. We can demonstrate the interface first and see how our standard aligns with its model-building process. Will there be any images being compressed like before? Just as you mentioned earlier, when a client has an image, it represents the data in tabular form. Yes, yes, yes; on web end primarily involves model management while server side is just about managing devices and servers. I get what you're saying. Right, we can do some modeling design here but not really on C-end or our local end since it's a bit complicated to implement there. Correct; everything related to the fields will be visible right from this interface.

The simplest action would be creating new fields in an entity and reverse-engineering models if needed. We don't have to build elements for that, and you can also ensure all tables and models we create are governed fields with strong tags. Yes indeed after completing the creation process, I mentioned earlier about a standardization procedure; when you click on it, there will be alignment done against our standards. When building models during this stage, new model strategies align to predefined standards ensuring consistency in scope of actions taken.

[Host]: During subsequent standardization processes, we can intelligently recommend relevant standards for your fields and send them over accordingly. You would then make the appropriate selections based on these recommendations.
[Zhang Manager]: Once a suitable standard is selected, when using related applications, our system automatically adjusts entity names and field lengths to conform with those specified requirements. After this adjustment process, we proceed with publishing steps before completing the entire standardization workflow.
[Host]: Regarding model analysis, we will focus primarily on studying differences between two versions of models through comparison for performance optimization purposes. During implementation phases such as落地分析(implementation), we can examine details like system entity representation rates and changes in various metrics including冠标率(guanbiao rate). For instance, the guanbiao rate includes both soft罐标(suangianbiao) which are fully consistent definitions due to some vendor-provided systems that cannot be changed; and hard罐标(haiguantiao), referring to aligning existing data with a target standard followed by implementing it in practical applications. The calculation of冠标率includes these two aspects ensuring 100% consistency between them.
[Zhang Manager]: Soft罐标means adjusting current formats using mappings so that they meet new requirements. Can you configure options via the tool for this purpose?
[Host]: Of course! During model standardization, we need to clearly define metrics regarding guanbiao or suangianbiao and use these as key indicators in measuring the degree of贯标(guanbiao).
[Host]: Next, let's focus on how our internal management system handles coordination among different modules. Additionally, should we consider incorporating data lineage analysis into discussions about procurement tools?
[Zhang Manager]: Indeed! Basic functions such as student information collection and foundational analytics have already been integrated within the original data management platform. These capabilities can support further development of advanced features in future projects.

[Host]: If the above plan is implemented, will we be able to effectively trace each field’s change path when applying models?

There's a dedicated module in our platform for this purpose. What are the differences between these two approaches? When online rates are high, you can perform real-time parsing with higher implementation rates. Complex lineage relationships can also be checked there. Currently, no data is being collected on that platform; we cannot see the bloodline information here yet. We only have a rough overview of simpler situations. Could I please look at any related case studies to show us an actual example of basic external deployment? I’ll wait for a moment as my network seems to be lagging slightly. This might involve data lineage issues between departments, and we would like to see if there are relevant examples because our scenarios may include two major ones:

The first scenario involves longitudinal data bloodline from the reporting end through the lakehouse system all the way up to the 400-system interface. If a problem with number quality is discovered at the reporting end, can you trace back quickly to find which tables and fields were involved in this process? There’s also a horizontal scenario where different systems use one field:

For example, when System A uses data from System B that then flows into System C for processing: if business needs call for new functionalities involving changes within certain system internals. Can we track the table and its related fields’ modifications through cross-system analysis to see which tables or columns will also be affected? In other words, can you show us whether those linked tables and fields are going to change as well?

Currently do you have any implementation cases of this horizontal bloodline relationship in real-world applications? Nanjing Bank has a bloodline system that allows for real-time cross-access impact analysis. Langfang Bank is also making similar attempts.

[Host]: Another type of traceability analysis can go back all the way to where exactly this field originated from, right? For influence analysis: you mentioned earlier how tracing data lineage by finding related tables and fields in a certain table.
[Zhang Manager]: Web services allow for automatic scanning. But we still need manual configuration one-by-one. Can API interfaces automatically scan web device interfaces as well?
[Host]: Can they be automatically replenished? Are you able to do this step, right? That’s the question being asked: can it be done automatically.
[Zhang Manager]: Yes, but a human will verify it. As long as there is logically coherent data that follows standard norms, these systems should be able to detect them automatically.
[Host]: Naming conventions and web service logic may still have some related aspects. Is your web service’s configuration something they need? Have you already configured this?
[Zhang Manager]: Yes, I don’t see any other issues here; let me continue with the current functionality, which is quite crucial for data governance.

[Host]: Since our models are source-managed from design-time to ensure standardized quality handling. If we define requirements early in logical and physical modeling…
[Zhang Manager]: So it’s important that we identify these data needs upfront. From this discussion earlier, I think a few points may be related to how you govern your data.
[Host]: Regarding compliance issues for risk control: can the existing systems address those? In terms of what they are designed to do?
[Zhang Manager]: We need to approach it in an scenario-based way. Currently we have Version 2.0 of our regulatory compliance system, which focuses on improving quality and adding pre-emptive measures.
[Host]: This is through a data governance platform; another aspect was the classification and grading for secure storage mentioned earlier about your asset management platforms?
[Zhang Manager]: Our Asset Management Platform covers these tools to facilitate business processes. How do we demonstrate what constitutes an asset? Well, I touched on this before but not with precise definitions.
[Host]: Regardless of whether insurance banks define it accurately or not: our metrics, tags, services and the models from the platform can all be considered assets in a way. So these are bridges between technical departments and business units—assuming that we use metrics as such a bridge…

[Zhang Manager]: Yes, no issues here; I’ll continue.

[Host]: How do you define your metrics? In what ways should they serve data assets to reflect their value?
[Zhang Manager]: Through the various means within our Data Governance Platform. From this perspective, it’s critical for classification and grading as well as establishing a clear directory of those data assets.

[Li Manager]:
I agree with General Zhang’s analysis, especially regarding risk control and data asset management in the business departments. This will bring significant value.

[Host]:
The third aspect of value is operational analytics. Demonstrating the value of data governance through operational analytics, and ensuring that metrics are at the core of this process, is critical to its success.

[General Zhang]:
I believe it’s crucial for us to focus on metric building in the next three to five years across the entire financial industry sector to ensure high-quality original data with standardized processing.

[Li Manager]:
Specifically speaking, good governance can enhance value through personnel management, finance analysis, strategic and marketing insights. This is one of my key conclusions from interpreting requirements.

[Host]:
Furthermore, integrating platforms and applications via a dedicated department for data control proves essential. Currently, we see that both warehouse (DW) and BI needs should be separated with platform construction and BI application leading the way.

[General Zhang]:
However, failing to achieve this in integrated management is problematic. The key lies in building an appropriate data architecture and model structure.

[Li Manager]:
When discussing a data architecture, standard definitions, modeling, and defining what constitutes a data asset are crucial aspects. This forms one of our foundational premises within the OITP (Operational Information & Technology Platform) framework.

[Host]:
If these components were to be handled separately, it would become challenging to ensure clear梳理和标准化管理原生数据， nor could we achieve standardized requirements management for data needs. Therefore, in order to address all issues through governance under the context of OITP, unifying solutions via governance is a necessary step.

[General Zhang]:
The main issue we’ve identified currently centers around deficiencies in our data platforms and BI applications as well as regulatory mechanisms, which reveal problems with current data control and management systems.

[Li Manager]:
Regardless of future tool choices for management, integration remains essential. Our recommendation involves providing foundational data services through the data governance department while combining metrics among other functions to support business requirement analysis work.

[Host]:
Previously passive manual service has transformed into asset-based data services. This can be seen via a data assets portal – this is precisely where our value in data governance lies.
 
[General Zhang]:
Yes, I approach it from three or five different perspectives: first by providing efficient data services; secondly using the data asset directory; third through metrics and operational analysis; fourth with applications like classification and grading of security. This truly highlights the importance and value of establishing a comprehensive system for managing data governance. Our platform can provide effective support and assurance in this process.

[Host]:
Could you elaborate on what these four critical aspects are, and how they help us achieve our goals?

[General Zhang]:
Through these methods, we clearly see the value of data governance and its crucial role in driving digital transformation within enterprises specifically. By leveraging services for metrics management, asset directory control, operational analytics, as well as measures like security classification grading, it enhances efficiency while reducing risks.

[Li Manager]:
I fully agree with General Zhang’s insights. Our platform has accumulated a large number of BI reports and historical datasets that serve as solid foundations for our future projects.
 
[Host]:
Now regarding the current system architecture we have in place – do you have specific implementation paths?

[General Zhang]:
Indeed, currently we already have regulatory compliance requirements along with related security classification analyses. Additionally, extensive experience has been accumulated in BI reports using both Click and BO tools which were implemented many years ago; a substantial amount of historical datasets is also available.

[Li Manager]:
Yes, over the past few years, significant efforts went into building our platform and warehouse systems resulting in an integrated yet mature data governance framework. However, we are aware that there could be some redundant processing – for instance, thousands or tens of thousands of metrics with a lot of duplicated information within them. This not only indicates issues related to data quality but also reveals problems during the process of governing our datasets.

[Host]:
Do you have any suggestions on how to improve this situation?

[General Zhang]:
Looking into the future, we need standardization in existing systems and add strict data quality checks at ETL scheduling times.

This will not only ensure that the newly generated standards are stored accurately in the database, but also help us manage the entire data asset lifecycle more effectively.
[Manager Li]: I agree with this view. By implementing these improvements, we can ensure future projects run more efficiently and reliably, thereby creating greater value for our company.

[Host]: This is related to the platform. Earlier you mentioned that the dataset was about BI, HR & Finance but not Strategy.
[Manager Zhang]: Suppose on your data platform there are different tables such as ODS table, DWD table, DWS Table and ADS table which are all part of a data mart (AADS), then it is possible to face duplication issues. If you include these tables in the Data Governance Platform or Asset Management Platform for management, we need ensure they’re managed properly.
[Manager Li]: If we don’t manage our existing datasets well, there’s no way that current problems can be solved. Two critical points are worth noting: one is the existing data platform itself has issues; two is currently dataset accuracy isn’t enough. When you mentioned how to solve this issue earlier, I think passive measures must be taken regarding the status quo. We should design solutions with applications in mind and drive improvements on our datasets as well as optimizing OATP.
[Doctor Wang]: It’s unrealistic to tackle these issues at their source because completely changing existing core systems can be very difficult or even impossible. Therefore, we suggest tackling this issue starting from data application and the current data platform, then gradually moving towards backend sources which is one of our best practices currently.

[Zhang Manager]: Based on statistics in China's large banks (such as Construction Bank or commercial banks), they usually have over 200 to 300 small-scale database systems; whereas smaller financial institutions may only have less than 100 datasets, and this has been the case for nearly 25 years. However, it is not related to OITP in terms of data management.
[Manager Li]: If we don’t solve problems at their source (OIAP), no matter how much effort we put into refining our standards and requirements definition, accuracy can never be guaranteed within business systems. The final issue that arises will be the erroneous information stored even in ODS; therefore, I believe more focus should be on data governance to improve overall quality of datasets as well as management processes.
[Host]: From this diagram, we clearly see the relationship between sources, platforms and applications which are closely linked providing an effective support mechanism for users. Teacher, please confirm if my understanding is correct? Thank you very much for your patience!

[Zhang Manager]: That would mean that it’s possible in such a case where some of our systems are products i.e., purchased from external vendors.
[Manager Li]: If so, they might face significant challenges?
[Engineer Wang]: In this situation, how do we generally handle things? Typically:

[Engineer Wang]: First, you need to design and plan the data architecture. Suppose your company’s core system was originally global and uses an AAKKG product from America.
[Zhang Manager]: Correct, so if RATP is part of that entire core system then it would be difficult to change?
[Manager Li]: Yes, in this case without establishing a new generation core system. So directly importing the 400-layer data into ODS database right? 
[Engineer Wang]: Indeed. In other words, the data within ODS is sourced from elsewhere.
[Zhang Manager]: If we only consider structured datasets and believe that these ODS datasets already meet standards, correct? Then based on this dataset for further work:
[Manager Li]: If the original ODS data doesn’t fit standards at all then there will be significant risks in terms of quality, security & classification analysis. 
[Zhang Manager]: Indeed so; it’s a deep-seated issue that can only be solved one by one.
[Engineer Wang]: Currently, banks have undergone revolutionary reforms during the construction of their new generation core systems:
[Manager Li]: They didn’t use TP and AP resources instead they simultaneously advanced data governance, standards & models which finally completed the building of this new system.

[President Zhang]:
The domestic insurance companies are lagging in this area. To be honest, that’s why the value of data governance lies in its being a continuous process.

[Engineer Wang]:
We advocate for an ongoing approach to data governance because only then can true progress truly be achieved. If there is no improvement on the TP side:

I have been deeply involved in business intelligence (BI), metrics, and data warehousing for over twenty years. Whether it's operational analysis or precise marketing,

[Manager Li]:
So, under this unified metric system framework, can we define atomic metrics, derived metrics and derivative metrics more clearly? This will help us better leverage BI tools for business analysis and decision support.

[Host]:
Regarding the issue of data application in your mentioned platform. The existing things are mostly historical rather than dynamic.
[Zhang Manager]:
The problem with a "dynamic warehouse" can be addressed through governance tools. Whether it's called a “real-time” or "dynamic" warehousing system, we need to solve for real-time warehouses and the unified metric systems along with BI issues that currently exist. Specifically speaking, assuming I see metrics data items in your tool interface before moving on to dedicated management pages.
[Zhang Manager]:
If I click into one of these metrics, it should clearly show which base data items are involved and allow connection association operations. However, what I mean is that there's a problem with the current datasets and metrics having names that either don't match or have different meanings across your existing ones. From an angle of view in terms of the data platform, we haven’t solved this issue at the tool level yet.
[Zhang Manager]:
If you can't change these datasets now, then our management has to be passive towards them. Many metrics related to business operations, finance and human resources are repetitive with name differences. How do we address that? We believe in establishing a data platform, it’s important to梳理 metric systems from the beginning.
[Zhang Manager]:
Through this process, more efficient management and application can be achieved at the tool level; otherwise, our management would become very passive. There is another issue I want to seek your advice on. This tool seems mainly focused on managing system-structured data issues, with some non-structured cases indeed being visible in terms of regulatory oversight.
[Zhang Manager]:
We hope that financial institutions will focus and manage non-structured data in the future as well. For now though, we are primarily focusing our efforts on structured data work. From a perspective of data governance, more needs to be explored regarding demand scenarios for unstructured data - such issues related to customer service or knowledge graph management continue to emerge.
[Zhang Manager]:
In summary, under current conditions, the tool mainly focuses on managing structured data. Thank you all professors if there are no other questions that need discussion...
[Host]: This meeting is now concluded. Thanks a lot for your participation and support.
[Professor A]: Thank you very much for sharing with us today.
[Professor B]: Thank you! Goodbye!
[Professor C]: Bye-bye!
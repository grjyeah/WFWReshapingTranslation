#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
FunASR WebUI - åŸºäºGradioçš„è¯­éŸ³è¯†åˆ«Webç•Œé¢
åŠŸèƒ½ï¼šè¯­éŸ³è¯†åˆ«ã€VADã€æ ‡ç‚¹æ¢å¤ã€è¯´è¯äººåˆ†ç¦»ã€æƒ…æ„Ÿè¯†åˆ«ç­‰
"""

import os
import sys
import json
import time
import traceback
from pathlib import Path
from typing import Dict, List, Tuple, Optional, Any

import gradio as gr
import numpy as np
import pandas as pd

# ç¡®ä¿å½“å‰ç›®å½•åœ¨Pythonè·¯å¾„ä¸­
current_dir = Path(__file__).parent
sys.path.insert(0, str(current_dir))

try:
    from funasr import AutoModel
    from funasr.utils.postprocess_utils import rich_transcription_postprocess
except ImportError as e:
    print(f"Error importing FunASR: {e}")
    print("Please install FunASR first: pip install funasr")
    sys.exit(1)

# å…¨å±€å˜é‡å­˜å‚¨å·²åŠ è½½çš„æ¨¡å‹
LOADED_MODELS = {}
MODEL_CONFIG = {
    "asr_models": {
        "SenseVoiceSmall": "iic/SenseVoiceSmall",
        "Paraformer-zh": "paraformer-zh",
        "Paraformer-zh-streaming": "paraformer-zh-streaming",
        "SeACoParaformer": "iic/speech_seaco_paraformer_large_asr_nat-zh-cn-16k-common-vocab8404-pytorch",
        "EngParaformer": "iic/speech_paraformer-large-vad-punc_asr_nat-en-16k-common-vocab10020",
        "Paraformer-en": "paraformer-en",
        "Conformer-en": "conformer-en",
        "Whisper-large-v3": "Whisper-large-v3",
        "Whisper-large-v3-turbo": "Whisper-large-v3-turbo"
    },
    "vad_models": {
        "None": None,
        "FSMN-VAD": "fsmn-vad"
    },
    "punc_models": {
        "None": None,
        "CT-Transformer": "ct-punc"
    },
    "spk_models": {
        "None": None,
        "CAM++": "cam++"
    },
    "emotion_models": {
        "None": None,
        "Emotion2Vec-Large": "emotion2vec_plus_large",
        "Emotion2Vec-Base": "emotion2vec_plus_base"
    }
}

LANGUAGE_OPTIONS = {
    "auto": "è‡ªåŠ¨æ£€æµ‹",
    "zh": "ä¸­æ–‡",
    "en": "è‹±æ–‡",
    "yue": "ç²¤è¯­",
    "ja": "æ—¥è¯­",
    "ko": "éŸ©è¯­"
}

# è¯­è¨€æ ‡ç­¾æ˜ å°„è¡¨
LANGUAGE_TAG_MAPPING = {
    "<|zh|>": "ä¸­æ–‡ï¼š",
    "<|en|>": "è‹±æ–‡ï¼š",
    "<|yue|>": "ç²¤è¯­ï¼š",
    "<|ja|>": "æ—¥è¯­ï¼š",
    "<|ko|>": "éŸ©è¯­ï¼š",
    "<|auto|>": "è‡ªåŠ¨æ£€æµ‹ï¼š"
}

def postprocess_sensevoice_result(text: str) -> str:
    """åå¤„ç†SenseVoiceSmallç»“æœï¼Œæ›¿æ¢è¯­è¨€æ ‡ç­¾ä¸ºå¯è¯»å‰ç¼€

    Args:
        text: åŸå§‹è¯†åˆ«æ–‡æœ¬ï¼Œå¯èƒ½åŒ…å«è¯­è¨€æ ‡ç­¾

    Returns:
        å¤„ç†åçš„æ–‡æœ¬ï¼Œè¯­è¨€æ ‡ç­¾è¢«æ›¿æ¢ä¸ºå¯è¯»å‰ç¼€å¹¶æ·»åŠ æ¢è¡Œ
    """
    if not text:
        return text

    import re

    # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼åŒ¹é…è¯­è¨€æ ‡ç­¾æ¨¡å¼
    # åŒ¹é… <|è¯­è¨€|> æ ¼å¼çš„æ ‡ç­¾
    pattern = r'<\|([^|]+)\|>'

    def replace_tag(match):
        language = match.group(1)
        tag = f"<|{language}|>"
        prefix = LANGUAGE_TAG_MAPPING.get(tag, f"{language}ï¼š")
        # ä¸ºç¬¬ä¸€ä¸ªæ ‡ç­¾å‰ä¸æ·»åŠ æ¢è¡Œï¼Œåç»­æ ‡ç­¾å‰æ·»åŠ æ¢è¡Œ
        if match.start() == 0:
            return f"{prefix} "
        else:
            return f"\n{prefix} "

    # æ‰§è¡Œæ›¿æ¢
    result = re.sub(pattern, replace_tag, text)

    # æ¸…ç†å¤šä½™çš„ç©ºæ ¼ï¼Œä½†ä¿ç•™æ¢è¡Œç¬¦
    # é¦–å…ˆå¤„ç†æ¢è¡Œç¬¦å‘¨å›´çš„ç©ºæ ¼
    result = re.sub(r'\s*\n\s*', '\n', result)  # æ¸…ç†æ¢è¡Œå‰åçš„ç©ºæ ¼
    # ç„¶åå¤„ç†ä¸è·¨æ¢è¡Œç¬¦çš„è¿ç»­ç©ºæ ¼
    result = re.sub(r'[^\S\r\n]+', ' ', result)  # åˆå¹¶éæ¢è¡Œç¬¦çš„ç©ºç™½å­—ç¬¦
    # æ¸…ç†æ¯è¡Œæœ«å°¾çš„ç©ºæ ¼
    result = re.sub(r' +\n', '\n', result)  # æ¸…ç†æ¢è¡Œå‰çš„ç©ºæ ¼
    result = result.strip()  # å»é™¤é¦–å°¾ç©ºç™½

    return result


class FunASRWebUI:
    def __init__(self):
        self.setup_environment()

    def get_model_capabilities(self, model_name: str) -> Dict[str, bool]:
        """è·å–æ¨¡å‹åŠŸèƒ½æ”¯æŒæƒ…å†µ"""
        # æ ¹æ®FunASRæ–‡æ¡£ï¼Œåªæœ‰è¿™ä¸¤ä¸ªæ¨¡å‹æ”¯æŒæ—¶é—´æˆ³é¢„æµ‹å’Œè¯´è¯äººåˆ†ç¦»
        timestamp_models = {
            "iic/speech_paraformer-large-vad-punc_asr_nat-zh-cn-16k-common-vocab8404-pytorch",
            "iic/speech_seaco_paraformer_large_asr_nat-zh-cn-16k-common-vocab8404-pytorch"
        }

        model_id = MODEL_CONFIG["asr_models"].get(model_name, "")

        return {
            "supports_timestamp": model_id in timestamp_models,
            "supports_speaker_diarization": model_id in timestamp_models,
            "supports_hotword": True,  # å¤§éƒ¨åˆ†æ¨¡å‹éƒ½æ”¯æŒçƒ­è¯
            "supports_multilingual": model_name in ["SenseVoiceSmall", "Whisper-large-v3", "Whisper-large-v3-turbo"],
            "model_id": model_id,
            "model_name": model_name
        }

    def get_optimal_device(self):
        """è·å–æœ€ä¼˜è®¡ç®—è®¾å¤‡"""
        try:
            import torch
            if torch.cuda.is_available():
                device_count = torch.cuda.device_count()
                print(f"ğŸ¯ æ£€æµ‹åˆ° {device_count} ä¸ªGPUè®¾å¤‡ï¼Œä¼˜å…ˆä½¿ç”¨GPU")
                return "cuda"
            elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
                print("ğŸ¯ æ£€æµ‹åˆ°MPSè®¾å¤‡ï¼Œä½¿ç”¨MPSåŠ é€Ÿ")
                return "mps"
            else:
                print("âš ï¸  æœªæ£€æµ‹åˆ°GPUè®¾å¤‡ï¼Œä½¿ç”¨CPU")
                return "cpu"
        except Exception as e:
            print(f"âš ï¸  è®¾å¤‡æ£€æµ‹å¤±è´¥: {e}ï¼Œå›é€€åˆ°CPU")
            return "cpu"

    def setup_environment(self):
        """è®¾ç½®ç¯å¢ƒå˜é‡"""
        os.environ.setdefault("HF_ENDPOINT", "https://hf-mirror.com")
        models_dir = current_dir / "models"
        models_dir.mkdir(exist_ok=True)

        hf_cache = models_dir / "huggingface"
        ms_cache = models_dir / "modelscope"
        hf_cache.mkdir(exist_ok=True)
        ms_cache.mkdir(exist_ok=True)

        os.environ["HF_HOME"] = str(hf_cache)
        os.environ["HUGGINGFACE_HUB_CACHE"] = str(hf_cache)
        os.environ["MODELSCOPE_CACHE"] = str(ms_cache)

    def load_model(self, model_key: str, model_config: Dict) -> Any:
        """åŠ è½½æ¨¡å‹"""
        if model_key in LOADED_MODELS:
            return LOADED_MODELS[model_key]

        try:
            # æ·»åŠ disable_update=Trueç¦ç”¨è‡ªåŠ¨æ›´æ–°æ£€æŸ¥
            model_config["disable_update"] = True
            model = AutoModel(**model_config)
            LOADED_MODELS[model_key] = model
            return model
        except Exception as e:
            raise Exception(f"åŠ è½½æ¨¡å‹å¤±è´¥: {str(e)}")

    def create_basic_asr_interface(self):
        """åˆ›å»ºåŸºç¡€è¯­éŸ³è¯†åˆ«ç•Œé¢"""
        with gr.Column():
            gr.Markdown("## ğŸ¤ åŸºç¡€è¯­éŸ³è¯†åˆ«")
            gr.Markdown("ä¸Šä¼ éŸ³é¢‘æ–‡ä»¶æˆ–å½•åˆ¶è¯­éŸ³è¿›è¡Œè¯†åˆ«")

            with gr.Row():
                with gr.Column(scale=2, min_width=400):
                    # è¾“å…¥åŒºåŸŸ
                    audio_input = gr.Audio(
                        label="éŸ³é¢‘è¾“å…¥",
                        type="filepath",
                        sources=["upload", "microphone"],
                        container=True
                    )

                    # æ¨¡å‹å’Œè¯­è¨€é€‰æ‹©ï¼ˆåœ¨ä¸€è¡Œä¸­ï¼‰
                    with gr.Row():
                        asr_model = gr.Dropdown(
                            choices=list(MODEL_CONFIG["asr_models"].keys()),
                            value="SeACoParaformer",  # é»˜è®¤ä½¿ç”¨SeACoParaformer
                            label="è¯­éŸ³è¯†åˆ«æ¨¡å‹",
                            scale=2
                        )

                        language = gr.Dropdown(
                            choices=[
                                ("è‡ªåŠ¨æ£€æµ‹", "auto"),
                                ("ä¸­æ–‡", "zh"),
                                ("è‹±æ–‡", "en"),
                                ("ç²¤è¯­", "yue"),
                                ("æ—¥è¯­", "ja"),
                                ("éŸ©è¯­", "ko")
                            ],
                            value="auto",
                            label="è¯­è¨€",
                            scale=1
                        )

                    # åŠŸèƒ½é€‰é¡¹ï¼ˆä¼˜åŒ–å¸ƒå±€ï¼‰
                    with gr.Row():
                        use_vad = gr.Checkbox(label="ä½¿ç”¨VAD", value=True)
                        use_punc = gr.Checkbox(label="ä½¿ç”¨æ ‡ç‚¹", value=True)
                        use_itn = gr.Checkbox(label="ä½¿ç”¨ITN", value=True)

                    # çƒ­è¯åŠŸèƒ½
                    with gr.Row():
                        use_hotword = gr.Checkbox(label="ä½¿ç”¨çƒ­è¯", value=False)

                    # çƒ­è¯æ–‡ä»¶ä¸Šä¼ ï¼ˆæ ¹æ®å¼€å…³æ˜¾ç¤º/éšè—ï¼‰
                    hotword_file = gr.File(
                        label="çƒ­è¯æ–‡ä»¶ï¼ˆå¯é€‰ï¼‰",
                        file_types=[".txt"],
                        type="filepath",
                        visible=False  # é»˜è®¤éšè—ï¼Œåªåœ¨éœ€è¦æ—¶æ˜¾ç¤º
                    )

                    # çƒ­è¯å¼€å…³
                    use_hotword = gr.Checkbox(label="ä½¿ç”¨çƒ­è¯", value=False)

                    # æ ¹æ®çƒ­è¯å¼€å…³æ˜¾ç¤º/éšè—æ–‡ä»¶ä¸Šä¼ 
                    def toggle_hotword_visibility(use_hotword):
                        return gr.File(visible=use_hotword)

                    use_hotword.change(
                        toggle_hotword_visibility,
                        inputs=[use_hotword],
                        outputs=[hotword_file]
                    )

                    # è¯†åˆ«æŒ‰é’®ï¼ˆå¢å¤§å°ºå¯¸ï¼‰
                    recognize_btn = gr.Button(
                        "ğŸš€ å¼€å§‹è¯†åˆ«",
                        variant="primary",
                        size="lg"
                    )

                with gr.Column(scale=3, min_width=500):
                    # è¾“å‡ºåŒºåŸŸï¼ˆå¢åŠ é—´è·ï¼‰
                    with gr.Group():
                        result_text = gr.Textbox(
                            label="è¯†åˆ«ç»“æœ",
                            lines=10,  # å¢åŠ è¡Œæ•°
                            placeholder="è¯†åˆ«ç»“æœå°†åœ¨è¿™é‡Œæ˜¾ç¤º...",
                            max_lines=20,
                            interactive=False,
                            container=True
                        )

                        status_info = gr.Textbox(
                            label="çŠ¶æ€ä¿¡æ¯",
                            lines=4,  # å¢åŠ è¡Œæ•°
                            placeholder="å‡†å¤‡å°±ç»ª",
                            interactive=False,
                            container=True
                        )

                    # ç»“æœè¯¦æƒ…ï¼ˆä¼˜åŒ–å±•ç¤ºï¼‰
                    with gr.Accordion("ğŸ“Š è¯¦ç»†ä¿¡æ¯", open=False):
                        result_json = gr.JSON(
                            label="å®Œæ•´ç»“æœ",
                            container=True
                        )

            # è¯†åˆ«éŸ³é¢‘å‡½æ•°
            def recognize_audio(audio_path, model_name, lang, vad, punc, itn, use_hotword, hotword_file=None):
                if not audio_path:
                    return "è¯·å…ˆä¸Šä¼ æˆ–å½•åˆ¶éŸ³é¢‘æ–‡ä»¶", "é”™è¯¯ï¼šæ²¡æœ‰éŸ³é¢‘è¾“å…¥", {}

                try:
                    status_msg = f"æ­£åœ¨åŠ è½½æ¨¡å‹ {model_name}..."
                    yield "", status_msg, {}

                    # æ„å»ºæ¨¡å‹é…ç½®
                    config = {
                        "model": MODEL_CONFIG["asr_models"][model_name],
                        "device": self.get_optimal_device()
                    }

                    # å¦‚æœæ˜¯SeACoParaformeræ¨¡å‹ï¼Œæ·»åŠ ç‰ˆæœ¬å·
                    if model_name == "SeACoParaformer":
                        config["model_revision"] = "v2.0.4"

                    if vad:
                        config["vad_model"] = "fsmn-vad"
                        config["vad_kwargs"] = {"max_single_segment_time": 30000}
                        # å¦‚æœæ˜¯SeACoParaformeræ¨¡å‹ï¼Œæ·»åŠ VADç‰ˆæœ¬å·
                        if model_name == "SeACoParaformer":
                            config["vad_model_revision"] = "v2.0.4"

                    if punc:
                        config["punc_model"] = "ct-punc"
                        # å¦‚æœæ˜¯SeACoParaformeræ¨¡å‹ï¼Œæ·»åŠ æ ‡ç‚¹ç‰ˆæœ¬å·
                        if model_name == "SeACoParaformer":
                            config["punc_model_revision"] = "v2.0.4"

                    # åŠ è½½æ¨¡å‹
                    model_key = f"{model_name}_vad-{vad}_punc-{punc}"
                    model = self.load_model(model_key, config)

                    status_msg = "æ­£åœ¨è¯†åˆ«ä¸­..."
                    yield "", status_msg, {}

                    # æ‰§è¡Œè¯†åˆ«
                    start_time = time.time()

                    # æ„å»ºç”Ÿæˆå‚æ•°
                    generate_kwargs = {
                        "input": audio_path,
                        "batch_size_s": 300,
                        "merge_vad": vad
                    }

                    # ä¸ºä¸åŒæ¨¡å‹æ·»åŠ ç‰¹å®šå‚æ•°
                    if model_name == "SenseVoiceSmall":
                        generate_kwargs.update({
                            "cache": {},
                            "language": lang,
                            "use_itn": itn,
                            "batch_size_s": 60,
                            "merge_length_s": 15
                        })
                    elif model_name.startswith("Whisper"):
                        # Whisperæ¨¡å‹ç‰¹æ®Šå¤„ç†
                        generate_kwargs["batch_size_s"] = 0  # è®¾ç½®ä¸º0ç¡®ä¿batch_size=1

                    # æ·»åŠ çƒ­è¯å‚æ•°ï¼ˆå¦‚æœå¯ç”¨äº†çƒ­è¯åŠŸèƒ½ï¼‰
                    if use_hotword and hotword_file and os.path.exists(hotword_file):
                        generate_kwargs["hotword"] = hotword_file
                        status_msg = "æ­£åœ¨è¯†åˆ«ä¸­ï¼ˆä½¿ç”¨çƒ­è¯ï¼‰..."
                        yield "", status_msg, {}

                    # æ‰§è¡Œè¯†åˆ«
                    res = model.generate(**generate_kwargs)

                    if res and len(res) > 0:
                        if model_name == "SenseVoiceSmall":
                            raw_text = rich_transcription_postprocess(res[0]["text"]) if res[0]["text"] else "æœªè¯†åˆ«åˆ°å†…å®¹"
                            text = postprocess_sensevoice_result(raw_text)
                        else:
                            text = res[0].get("text", "æœªè¯†åˆ«åˆ°å†…å®¹")
                        result_detail = res[0]
                    else:
                        text = "æœªè¯†åˆ«åˆ°å†…å®¹"
                        result_detail = {}

                    end_time = time.time()
                    duration = end_time - start_time

                    status_msg = f"è¯†åˆ«å®Œæˆï¼è€—æ—¶: {duration:.2f}ç§’"
                    if use_hotword and hotword_file:
                        status_msg += "ï¼ˆä½¿ç”¨çƒ­è¯ï¼‰"

                    yield text, status_msg, result_detail

                except Exception as e:
                    error_msg = f"è¯†åˆ«å¤±è´¥: {str(e)}"

                    # æ£€æŸ¥æ˜¯å¦æ˜¯æ¨¡å‹ç›¸å…³çš„é”™è¯¯
                    if "timestamp" in str(e).lower() or "speaker" in str(e).lower():
                        if model_name == "SenseVoiceSmall":
                            error_msg = f"è¯†åˆ«å¤±è´¥: SenseVoiceSmallæ¨¡å‹é‡åˆ°äº†é…ç½®é—®é¢˜ã€‚\nå»ºè®®ï¼šæ£€æŸ¥é«˜çº§åŠŸèƒ½è®¾ç½®æˆ–å°è¯•å…¶ä»–æ¨¡å‹ã€‚\nè¯¦ç»†é”™è¯¯: {str(e)}"
                        else:
                            error_msg = f"è¯†åˆ«å¤±è´¥: {model_name}æ¨¡å‹é‡åˆ°äº†é…ç½®é—®é¢˜ã€‚\nè¯¦ç»†é”™è¯¯: {str(e)}"

                    yield "", error_msg, {"error": str(e)}

            # æŒ‰é’®ç‚¹å‡»äº‹ä»¶
            recognize_btn.click(
                recognize_audio,
                inputs=[audio_input, asr_model, language, use_vad, use_punc, use_itn, use_hotword, hotword_file],
                outputs=[result_text, status_info, result_json]
            )

        return audio_input, asr_model, language, use_vad, use_punc, use_itn, use_hotword, hotword_file, recognize_btn, result_text, status_info, result_json

    def create_advanced_interface(self):
        """åˆ›å»ºé«˜çº§åŠŸèƒ½ç•Œé¢"""
        with gr.Column():
            gr.Markdown("## âš™ï¸ é«˜çº§åŠŸèƒ½")
            gr.Markdown("åŒ…å«è¯´è¯äººåˆ†ç¦»ã€æƒ…æ„Ÿè¯†åˆ«ã€æ—¶é—´æˆ³ç­‰é«˜çº§åŠŸèƒ½")

            with gr.Row():
                with gr.Column(scale=2, min_width=450):
                    # éŸ³é¢‘è¾“å…¥
                    audio_input_adv = gr.Audio(
                        label="éŸ³é¢‘è¾“å…¥",
                        type="filepath",
                        sources=["upload", "microphone"],
                        container=True
                    )

                    # æ¨¡å‹é€‰æ‹©ç»„ï¼ˆä¼˜åŒ–å¸ƒå±€ï¼‰
                    with gr.Group():
                        gr.Markdown("### ğŸ¤– æ¨¡å‹é€‰æ‹©")

                        # ASRå’ŒVADæ¨¡å‹åœ¨ä¸€è¡Œ
                        with gr.Row():
                            asr_model_adv = gr.Dropdown(
                                choices=list(MODEL_CONFIG["asr_models"].keys()),
                                value="SeACoParaformer",  # é»˜è®¤ä½¿ç”¨SeACoParaformer
                                label="ASRæ¨¡å‹",
                                scale=2
                            )
                            vad_model_adv = gr.Dropdown(
                                choices=list(MODEL_CONFIG["vad_models"].keys()),
                                value="FSMN-VAD",
                                label="VADæ¨¡å‹",
                                scale=1
                            )

                        # æ ‡ç‚¹å’Œè¯´è¯äººæ¨¡å‹åœ¨ä¸€è¡Œ
                        with gr.Row():
                            punc_model_adv = gr.Dropdown(
                                choices=list(MODEL_CONFIG["punc_models"].keys()),
                                value="CT-Transformer",
                                label="æ ‡ç‚¹æ¨¡å‹",
                                scale=1
                            )
                            spk_model_adv = gr.Dropdown(
                                choices=list(MODEL_CONFIG["spk_models"].keys()),
                                value="CAM++",
                                label="è¯´è¯äººæ¨¡å‹",
                                scale=1
                            )

                    # çƒ­è¯åŠŸèƒ½ï¼ˆé«˜çº§ç•Œé¢ï¼‰
                    with gr.Group():
                        gr.Markdown("### ğŸ”¥ğŸ”¥ çƒ­è¯åŠŸèƒ½")

                        with gr.Row():
                            use_hotword_adv = gr.Checkbox(label="ä½¿ç”¨çƒ­è¯", value=False)

                        hotword_file_adv = gr.File(
                            label="çƒ­è¯æ–‡ä»¶ï¼ˆå¯é€‰ï¼‰",
                            file_types=[".txt"],
                            type="filepath",
                            visible=False
                        )

                        # æ ¹æ®çƒ­è¯å¼€å…³æ˜¾ç¤º/éšè—æ–‡ä»¶ä¸Šä¼ 
                        def toggle_hotword_visibility_adv(use_hotword):
                            return gr.File(visible=use_hotword)

                        use_hotword_adv.change(
                            toggle_hotword_visibility_adv,
                            inputs=[use_hotword_adv],
                            outputs=[hotword_file_adv]
                        )

                    # é«˜çº§å‚æ•°ï¼ˆä¼˜åŒ–å¸ƒå±€ï¼‰
                    with gr.Group():
                        gr.Markdown("### âš™ï¸ é«˜çº§å‚æ•°")

                        # æ•°å€¼å‚æ•°åœ¨ä¸€è¡Œ
                        with gr.Row():
                            batch_size = gr.Slider(
                                60, 600, value=300, step=60,
                                label="æ‰¹å¤„ç†å¤§å°(ç§’)",
                                interactive=True
                            )
                            merge_length = gr.Slider(
                                5, 30, value=15, step=5,
                                label="VADåˆå¹¶é•¿åº¦(ç§’)",
                                interactive=True
                            )

                        # åŠŸèƒ½å¼€å…³åœ¨ä¸€è¡Œ
                        with gr.Row():
                            return_timestamps = gr.Checkbox(
                                label="è¿”å›æ—¶é—´æˆ³", value=True
                            )
                            sentence_timestamp = gr.Checkbox(
                                label="å¥å­çº§æ—¶é—´æˆ³", value=True
                            )
                            return_spk_res = gr.Checkbox(
                                label="è¿”å›è¯´è¯äººç»“æœ", value=True
                            )

                        # æ¨¡å‹åŠŸèƒ½æç¤º
                        model_capability_info = gr.Textbox(
                            label="æ¨¡å‹åŠŸèƒ½æç¤º",
                            lines=2,
                            interactive=False,
                            container=True
                        )

                    # å¤„ç†æŒ‰é’®ï¼ˆå¢å¤§å°ºå¯¸ï¼‰
                    process_adv_btn = gr.Button(
                        "ğŸ”§ å¼€å§‹é«˜çº§å¤„ç†",
                        variant="primary",
                        size="lg"
                    )

                with gr.Column(scale=3, min_width=600):
                    # ç»“æœæ˜¾ç¤ºåŒºåŸŸï¼ˆæ”¹è¿›æ ‡ç­¾é¡µï¼‰
                    result_tabs = gr.Tabs()

                    with result_tabs:
                        with gr.TabItem("ğŸ“ æ–‡æœ¬ç»“æœ"):
                            text_result_adv = gr.Textbox(
                                label="è¯†åˆ«æ–‡æœ¬",
                                lines=8,
                                placeholder="è¯†åˆ«ç»“æœ...",
                                max_lines=20,
                                interactive=False,
                                container=True
                            )

                        with gr.TabItem("â° æ—¶é—´æˆ³"):
                            timestamp_result = gr.Dataframe(
                                headers=["å¼€å§‹æ—¶é—´", "ç»“æŸæ—¶é—´", "æ–‡æœ¬"],
                                label="æ—¶é—´æˆ³ä¿¡æ¯",
                                interactive=False
                            )

                        with gr.TabItem("ğŸ‘¥ è¯´è¯äººåˆ†ç¦»"):
                            speaker_result = gr.Dataframe(
                                headers=["è¯´è¯äºº", "å¼€å§‹æ—¶é—´", "ç»“æŸæ—¶é—´", "æ–‡æœ¬"],
                                label="è¯´è¯äººåˆ†ç¦»ç»“æœ",
                                interactive=False
                            )

                            # ä¸€é”®å¤åˆ¶åŠŸèƒ½
                            with gr.Row():
                                copy_btn = gr.Button(" ä¸€é”®å¤åˆ¶è¡¨æ ¼", variant="primary")

                            export_output = gr.Textbox(
                                label="å¤åˆ¶ç»“æœ",
                                lines=5,
                                show_copy_button=True  # è¿™ä¸ªå±æ€§ä¼šæ˜¾ç¤ºå¤åˆ¶æŒ‰é’®
                            )

                            def copy_speaker_results(speaker_data):
                                if speaker_data is None or speaker_data.empty:
                                    return "æ²¡æœ‰æ•°æ®å¯å¤åˆ¶"

                                # ä½¿ç”¨åˆ¶è¡¨ç¬¦åˆ†éš”çš„æ ¼å¼ï¼Œæ–¹ä¾¿Excelè‡ªåŠ¨åˆ†åˆ—
                                text = "è¯´è¯äºº\tå¼€å§‹æ—¶é—´\tç»“æŸæ—¶é—´\tæ–‡æœ¬å†…å®¹\n"

                                for index, row in speaker_data.iterrows():
                                    speaker = str(row['è¯´è¯äºº']).replace('\t', ' ')
                                    start_time = str(row['å¼€å§‹æ—¶é—´']).replace('\t', ' ')
                                    end_time = str(row['ç»“æŸæ—¶é—´']).replace('\t', ' ')
                                    # æ–‡æœ¬åˆ—å·²ç»åŒ…å«ã€è¯´è¯äººXã€‘æ ‡ç­¾ï¼Œç›´æ¥ä½¿ç”¨å³å¯
                                    content = str(row['æ–‡æœ¬']).replace('\t', ' ').replace('\n', ' ')

                                    text += f"{speaker}\t{start_time}\t{end_time}\t{content}\n"

                                return text

                            copy_btn.click(copy_speaker_results, inputs=[speaker_result], outputs=[export_output])

                        with gr.TabItem("ğŸ“Š å®Œæ•´ç»“æœ"):
                            full_result_adv = gr.JSON(
                                label="å®Œæ•´ç»“æœ",
                                container=True
                            )

                    # çŠ¶æ€ä¿¡æ¯ï¼ˆæ”¾åœ¨æ ‡ç­¾é¡µä¸‹æ–¹ï¼‰
                    status_adv = gr.Textbox(
                        label="å¤„ç†çŠ¶æ€",
                        lines=4,
                        placeholder="å‡†å¤‡å°±ç»ª",
                        interactive=False,
                        container=True
                    )

            # æ›´æ–°æ¨¡å‹åŠŸèƒ½æç¤º
            def update_model_capability_info(model_name):
                capabilities = self.get_model_capabilities(model_name)
                info_parts = []

                if not capabilities["supports_timestamp"]:
                    info_parts.append("âš ï¸ ä¸æ”¯æŒæ—¶é—´æˆ³é¢„æµ‹")
                if not capabilities["supports_speaker_diarization"]:
                    info_parts.append("âš ï¸ ä¸æ”¯æŒè¯´è¯äººåˆ†ç¦»")
                if capabilities["supports_multilingual"]:
                    info_parts.append("âœ… æ”¯æŒå¤šè¯­è¨€è¯†åˆ«")
                if capabilities["supports_hotword"]:
                    info_parts.append("âœ… æ”¯æŒçƒ­è¯åŠŸèƒ½")

                if info_parts:
                    return " | ".join(info_parts)
                else:
                    return "âœ… æ”¯æŒæ‰€æœ‰é«˜çº§åŠŸèƒ½"

            # é«˜çº§å¤„ç†å‡½æ•°
            def process_advanced(audio_path, asr_model, vad_model, punc_model, spk_model,
                                 batch_size, merge_length, timestamps, sent_timestamps, spk_res,
                                 use_hotword, hotword_file):
                if not audio_path:
                    return "è¯·ä¸Šä¼ éŸ³é¢‘æ–‡ä»¶", pd.DataFrame(), pd.DataFrame(), {}, "é”™è¯¯ï¼šæ²¡æœ‰éŸ³é¢‘è¾“å…¥"

                try:
                    yield "", pd.DataFrame(), pd.DataFrame(), {}, "æ­£åœ¨åŠ è½½æ¨¡å‹..."

                    # æ„å»ºæ¨¡å‹é…ç½®
                    config = {
                        "model": MODEL_CONFIG["asr_models"][asr_model],
                        "device": self.get_optimal_device()
                    }

                    # å¦‚æœæ˜¯SeACoParaformeræ¨¡å‹ï¼Œæ·»åŠ ç‰ˆæœ¬å·
                    if asr_model == "SeACoParaformer":
                        config["model_revision"] = "v2.0.4"

                    if vad_model != "None":
                        config["vad_model"] = MODEL_CONFIG["vad_models"][vad_model]
                        config["vad_kwargs"] = {"max_single_segment_time": 30000}
                        if asr_model == "SeACoParaformer":
                            config["vad_model_revision"] = "v2.0.4"

                    if punc_model != "None":
                        config["punc_model"] = MODEL_CONFIG["punc_models"][punc_model]
                        if asr_model == "SeACoParaformer":
                            config["punc_model_revision"] = "v2.0.4"

                    if spk_model != "None":
                        config["spk_model"] = MODEL_CONFIG["spk_models"][spk_model]

                    # åŠ è½½æ¨¡å‹
                    model_key = f"adv_{asr_model}_{vad_model}_{punc_model}_{spk_model}"
                    model = self.load_model(model_key, config)

                    yield "", pd.DataFrame(), pd.DataFrame(), {}, "æ­£åœ¨å¤„ç†éŸ³é¢‘..."

                    # æ„å»ºç”Ÿæˆå‚æ•°
                    generate_kwargs = {
                        "input": audio_path,
                        "batch_size_s": batch_size,
                        "merge_vad": True,
                        "merge_length_s": merge_length
                    }

                    # Whisperæ¨¡å‹ç‰¹æ®Šå¤„ç†
                    if asr_model.startswith("Whisper"):
                        generate_kwargs["batch_size_s"] = 0

                    # æ£€æŸ¥æ¨¡å‹èƒ½åŠ›
                    capabilities = self.get_model_capabilities(asr_model)

                    # æ£€æŸ¥æ˜¯å¦å°è¯•å¯¹ä¸æ”¯æŒæ—¶é—´æˆ³çš„æ¨¡å‹ä½¿ç”¨æ—¶é—´æˆ³åŠŸèƒ½
                    if not capabilities["supports_timestamp"] and (sent_timestamps or spk_res):
                        if asr_model == "SenseVoiceSmall":
                            warning_msg = "SenseVoiceSmallæ¨¡å‹ä¸æ”¯æŒæ—¶é—´æˆ³é¢„æµ‹å’Œè¯´è¯äººåˆ†ç¦»åŠŸèƒ½ï¼Œå°†ä½¿ç”¨åŸºç¡€è¯†åˆ«æ¨¡å¼"
                        else:
                            warning_msg = f"{asr_model}æ¨¡å‹ä¸æ”¯æŒæ—¶é—´æˆ³é¢„æµ‹å’Œè¯´è¯äººåˆ†ç¦»åŠŸèƒ½ï¼Œå°†ä½¿ç”¨åŸºç¡€è¯†åˆ«æ¨¡å¼"

                        yield warning_msg, pd.DataFrame(), pd.DataFrame(), {}, "æ­£åœ¨å¤„ç†éŸ³é¢‘..."
                        # ä¸æ·»åŠ ä¸æ”¯æŒçš„åŠŸèƒ½å‚æ•°
                    elif capabilities["supports_timestamp"]:
                        # åªæœ‰æ”¯æŒçš„æ¨¡å‹æ‰æ·»åŠ è¿™äº›å‚æ•°
                        generate_kwargs["sentence_timestamp"] = sent_timestamps
                        generate_kwargs["return_spk_res"] = spk_res

                    # æ·»åŠ çƒ­è¯å‚æ•°ï¼ˆå¦‚æœå¯ç”¨äº†çƒ­è¯åŠŸèƒ½ï¼‰
                    if use_hotword and hotword_file and os.path.exists(hotword_file):
                        generate_kwargs["hotword"] = hotword_file
                        yield "", pd.DataFrame(), pd.DataFrame(), {}, "æ­£åœ¨å¤„ç†éŸ³é¢‘ï¼ˆä½¿ç”¨çƒ­è¯ï¼‰..."

                    # æ‰§è¡Œè¯†åˆ«
                    res = model.generate(**generate_kwargs)

                    if not res or len(res) == 0:
                        return "æœªè¯†åˆ«åˆ°å†…å®¹", pd.DataFrame(), pd.DataFrame(), {}, "è¯†åˆ«å®Œæˆï¼Œä½†æœªæ‰¾åˆ°æœ‰æ•ˆå†…å®¹"

                    result = res[0]
                    raw_text = result.get("text", "")

                    # å¯¹SenseVoiceSmallç»“æœè¿›è¡Œåå¤„ç†
                    if asr_model == "SenseVoiceSmall":
                        text = postprocess_sensevoice_result(raw_text)
                    else:
                        text = raw_text

                    # å¤„ç†æ—¶é—´æˆ³ - åªæœ‰æ”¯æŒçš„æ¨¡å‹æ‰ä¼šåŒ…å«æ—¶é—´æˆ³ä¿¡æ¯
                    timestamp_df = pd.DataFrame()
                    capabilities = self.get_model_capabilities(asr_model)

                    if capabilities["supports_timestamp"] and "timestamp" in result and result["timestamp"]:
                        timestamp_data = []
                        for ts in result["timestamp"]:
                            start_time = f"{ts[0] / 1000:.2f}s"
                            end_time = f"{ts[1] / 1000:.2f}s"
                            word = ts[2] if len(ts) > 2 else ""
                            timestamp_data.append([start_time, end_time, word])
                        timestamp_df = pd.DataFrame(timestamp_data, columns=["å¼€å§‹æ—¶é—´", "ç»“æŸæ—¶é—´", "æ–‡æœ¬"])
                    elif not capabilities["supports_timestamp"] and (sent_timestamps or spk_res):
                        # ä¸ºä¸æ”¯æŒçš„æ¨¡å‹åˆ›å»ºæç¤ºä¿¡æ¯
                        timestamp_df = pd.DataFrame([["ä¸æ”¯æŒ", "ä¸æ”¯æŒ", "å½“å‰æ¨¡å‹ä¸æ”¯æŒæ—¶é—´æˆ³åŠŸèƒ½"]],
                                                   columns=["å¼€å§‹æ—¶é—´", "ç»“æŸæ—¶é—´", "æ–‡æœ¬"])

                    # å¤„ç†è¯´è¯äººåˆ†ç¦» - åªæœ‰æ”¯æŒçš„æ¨¡å‹æ‰ä¼šåŒ…å«è¯´è¯äººä¿¡æ¯
                    speaker_df = pd.DataFrame()
                    if capabilities["supports_speaker_diarization"] and "sentence_info" in result and result["sentence_info"]:
                        speaker_data = []
                        for sent in result["sentence_info"]:
                            speaker = sent.get("spk", "æœªçŸ¥")
                            start_time = f"{sent.get('start', 0) / 1000:.2f}s"
                            end_time = f"{sent.get('end', 0) / 1000:.2f}s"
                            sentence = sent.get("sentence", "")
                            if not sentence:
                                sentence = sent.get("text", "")
                            if not sentence:
                                timestamp_text = sent.get("timestamp", [])
                                if timestamp_text and len(timestamp_text) > 0:
                                    sentence = " ".join([ts[2] for ts in timestamp_text if len(ts) > 2])

                            # åœ¨æ–‡æœ¬å‰é¢æ·»åŠ è¯´è¯äººæ ‡ç­¾
                            sentence_with_label = f"ã€è¯´è¯äºº:{speaker}ã€‘{sentence}"
                            speaker_data.append([speaker, start_time, end_time, sentence_with_label])
                        speaker_df = pd.DataFrame(speaker_data, columns=["è¯´è¯äºº", "å¼€å§‹æ—¶é—´", "ç»“æŸæ—¶é—´", "æ–‡æœ¬"])
                    elif not capabilities["supports_speaker_diarization"] and spk_res:
                        # ä¸ºä¸æ”¯æŒçš„æ¨¡å‹åˆ›å»ºæç¤ºä¿¡æ¯
                        speaker_df = pd.DataFrame([["ä¸æ”¯æŒ", "ä¸æ”¯æŒ", "ä¸æ”¯æŒ", "å½“å‰æ¨¡å‹ä¸æ”¯æŒè¯´è¯äººåˆ†ç¦»åŠŸèƒ½"]],
                                                  columns=["è¯´è¯äºº", "å¼€å§‹æ—¶é—´", "ç»“æŸæ—¶é—´", "æ–‡æœ¬"])

                    status_msg = "å¤„ç†å®Œæˆï¼"
                    if use_hotword and hotword_file:
                        status_msg += "ï¼ˆä½¿ç”¨çƒ­è¯ï¼‰"

                    yield text, timestamp_df, speaker_df, result, status_msg

                except Exception as e:
                    error_msg = f"å¤„ç†å¤±è´¥: {str(e)}"

                    # æ£€æŸ¥æ˜¯å¦æ˜¯SenseVoiceSmallç›¸å…³çš„é”™è¯¯
                    if "timestamp" in str(e).lower() and "speaker" in str(e).lower():
                        if asr_model == "SenseVoiceSmall":
                            error_msg = f"å¤„ç†å¤±è´¥: SenseVoiceSmallæ¨¡å‹ä¸æ”¯æŒæ—¶é—´æˆ³å’Œè¯´è¯äººåˆ†ç¦»åŠŸèƒ½ã€‚è¯·åœ¨é«˜çº§å‚æ•°ä¸­ç¦ç”¨è¿™äº›åŠŸèƒ½åé‡è¯•ã€‚\nè¯¦ç»†é”™è¯¯: {str(e)}"
                        else:
                            error_msg = f"å¤„ç†å¤±è´¥: {asr_model}æ¨¡å‹ä¸æ”¯æŒæ—¶é—´æˆ³å’Œè¯´è¯äººåˆ†ç¦»åŠŸèƒ½ã€‚è¯·åœ¨é«˜çº§å‚æ•°ä¸­ç¦ç”¨è¿™äº›åŠŸèƒ½åé‡è¯•ã€‚\nè¯¦ç»†é”™è¯¯: {str(e)}"

                    yield "", pd.DataFrame(), pd.DataFrame(), {"error": str(e)}, error_msg

            # é«˜çº§å¤„ç†æŒ‰é’®ç‚¹å‡»äº‹ä»¶
            process_adv_btn.click(
                process_advanced,
                inputs=[audio_input_adv, asr_model_adv, vad_model_adv, punc_model_adv, spk_model_adv,
                        batch_size, merge_length, return_timestamps, sentence_timestamp, return_spk_res,
                        use_hotword_adv, hotword_file_adv],
                outputs=[text_result_adv, timestamp_result, speaker_result, full_result_adv, status_adv]
            )

            # æ¨¡å‹é€‰æ‹©å˜åŒ–æ—¶æ›´æ–°åŠŸèƒ½æç¤º
            asr_model_adv.change(
                update_model_capability_info,
                inputs=[asr_model_adv],
                outputs=[model_capability_info]
            )

            # åˆå§‹åŒ–æ¨¡å‹åŠŸèƒ½æç¤º
            model_capability_info.value = update_model_capability_info("SeACoParaformer")

    def create_batch_interface(self):
        """åˆ›å»ºæ‰¹é‡å¤„ç†ç•Œé¢"""
        with gr.Column():
            gr.Markdown("## ğŸ“ æ‰¹é‡å¤„ç†")
            gr.Markdown("æ‰¹é‡å¤„ç†å¤šä¸ªéŸ³é¢‘æ–‡ä»¶")

            with gr.Row():
                with gr.Column(scale=1):
                    file_upload = gr.File(
                        label="é€‰æ‹©å¤šä¸ªéŸ³é¢‘æ–‡ä»¶",
                        file_count="multiple",
                        file_types=["audio"]
                    )

                    batch_model = gr.Dropdown(
                        choices=list(MODEL_CONFIG["asr_models"].keys()),
                        value="SenseVoiceSmall",
                        label="æ‰¹å¤„ç†æ¨¡å‹"
                    )

                    batch_options = gr.CheckboxGroup(
                        choices=["ä½¿ç”¨VAD", "ä½¿ç”¨æ ‡ç‚¹", "åŒ…å«æ—¶é—´æˆ³"],
                        value=["ä½¿ç”¨VAD", "ä½¿ç”¨æ ‡ç‚¹"],
                        label="å¤„ç†é€‰é¡¹"
                    )

                    batch_btn = gr.Button("ğŸš€ å¼€å§‹æ‰¹é‡å¤„ç†", variant="primary")

                with gr.Column(scale=2):
                    batch_progress = gr.Textbox(
                        label="å¤„ç†è¿›åº¦",
                        lines=3,
                        placeholder="ç­‰å¾…å¼€å§‹..."
                    )

                    batch_results = gr.Dataframe(
                        headers=["æ–‡ä»¶å", "çŠ¶æ€", "è¯†åˆ«ç»“æœ", "å¤„ç†æ—¶é—´"],
                        label="æ‰¹å¤„ç†ç»“æœ"
                    )

                    download_btn = gr.DownloadButton(
                        label="ğŸ“¥ ä¸‹è½½ç»“æœ(CSV)",
                        visible=False
                    )

        def batch_process(files, model_name, options):
            if not files:
                return "è¯·é€‰æ‹©è¦å¤„ç†çš„æ–‡ä»¶", pd.DataFrame(), gr.DownloadButton(visible=False)

            try:
                use_vad = "ä½¿ç”¨VAD" in options
                use_punc = "ä½¿ç”¨æ ‡ç‚¹" in options
                use_timestamps = "åŒ…å«æ—¶é—´æˆ³" in options

                # åŠ è½½æ¨¡å‹
                config = {
                    "model": MODEL_CONFIG["asr_models"][model_name],
                    "device": self.get_optimal_device()
                }

                if use_vad:
                    config["vad_model"] = "fsmn-vad"
                if use_punc:
                    config["punc_model"] = "ct-punc"

                model_key = f"batch_{model_name}_vad-{use_vad}_punc-{use_punc}"
                model = self.load_model(model_key, config)

                results = []
                total_files = len(files)

                for i, file in enumerate(files):
                    yield f"å¤„ç†ä¸­ {i+1}/{total_files}: {file.name}", pd.DataFrame(results), gr.DownloadButton(visible=False)

                    try:
                        start_time = time.time()

                        # æ‰§è¡Œè¯†åˆ«
                        if model_name.startswith("Whisper"):
                            # Whisperæ¨¡å‹ä¸æ”¯æŒbatchå¤„ç†ï¼Œè®¾ç½®batch_size_s=0
                            res = model.generate(
                                input=file.name,
                                batch_size_s=0,  # å…³é”®ä¿®å¤ï¼šè®¾ç½®ä¸º0ç¡®ä¿batch_size=1
                                sentence_timestamp=use_timestamps
                            )
                        elif model_name == "SenseVoiceSmall":
                            # SenseVoiceSmallæ¨¡å‹ä¸æ”¯æŒæ—¶é—´æˆ³åŠŸèƒ½ï¼Œæ£€æŸ¥ç”¨æˆ·è¯·æ±‚
                            capabilities = self.get_model_capabilities(model_name)
                            if use_timestamps and not capabilities["supports_timestamp"]:
                                # å¦‚æœç”¨æˆ·è¯·æ±‚äº†æ—¶é—´æˆ³ï¼Œç»™å‡ºè­¦å‘Šä½†ç»§ç»­å¤„ç†
                                print(f"è­¦å‘Šï¼šSenseVoiceSmallæ¨¡å‹ä¸æ”¯æŒæ—¶é—´æˆ³åŠŸèƒ½ï¼Œå°†ä½¿ç”¨åŸºç¡€è¯†åˆ«æ¨¡å¼å¤„ç†æ–‡ä»¶: {file.name}")
                            res = model.generate(
                                input=file.name,
                                batch_size_s=60,  # SenseVoiceSmallæ¨èçš„æ‰¹å¤„ç†å¤§å°
                                merge_length_s=15
                            )
                        else:
                            # å…¶ä»–æ¨¡å‹ï¼ˆåŒ…æ‹¬SeACoParaformerç­‰æ”¯æŒæ—¶é—´æˆ³çš„æ¨¡å‹ï¼‰
                            capabilities = self.get_model_capabilities(model_name)
                            if capabilities["supports_timestamp"]:
                                # æ”¯æŒæ—¶é—´æˆ³çš„æ¨¡å‹
                                res = model.generate(
                                    input=file.name,
                                    batch_size_s=300,
                                    sentence_timestamp=use_timestamps
                                )
                            else:
                                # ä¸æ”¯æŒæ—¶é—´æˆ³çš„æ¨¡å‹
                                if use_timestamps:
                                    print(f"è­¦å‘Šï¼š{model_name}æ¨¡å‹ä¸æ”¯æŒæ—¶é—´æˆ³åŠŸèƒ½ï¼Œå°†ä½¿ç”¨åŸºç¡€è¯†åˆ«æ¨¡å¼å¤„ç†æ–‡ä»¶: {file.name}")
                                res = model.generate(
                                    input=file.name,
                                    batch_size_s=300
                                )

                        end_time = time.time()
                        processing_time = f"{end_time - start_time:.2f}s"

                        if res and len(res) > 0:
                            raw_text = res[0].get("text", "æœªè¯†åˆ«åˆ°å†…å®¹")
                            if model_name == "SenseVoiceSmall":
                                text = postprocess_sensevoice_result(raw_text)
                            else:
                                text = raw_text
                            status = "æˆåŠŸ"
                        else:
                            text = "æœªè¯†åˆ«åˆ°å†…å®¹"
                            status = "æ— å†…å®¹"

                        results.append([
                            file.name.split("/")[-1],
                            status,
                            text[:100] + "..." if len(text) > 100 else text,
                            processing_time
                        ])

                    except Exception as e:
                        error_text = f"é”™è¯¯: {str(e)}"

                        # æ£€æŸ¥æ˜¯å¦æ˜¯æ—¶é—´æˆ³æˆ–è¯´è¯äººåˆ†ç¦»ç›¸å…³çš„é”™è¯¯
                        if "timestamp" in str(e).lower() or "speaker" in str(e).lower():
                            if model_name == "SenseVoiceSmall":
                                error_text = f"é”™è¯¯: SenseVoiceSmallæ¨¡å‹ä¸æ”¯æŒæ—¶é—´æˆ³åŠŸèƒ½ã€‚è¯·å–æ¶ˆå‹¾é€‰'åŒ…å«æ—¶é—´æˆ³'é€‰é¡¹åé‡è¯•ã€‚"
                            elif not self.get_model_capabilities(model_name)["supports_timestamp"]:
                                error_text = f"é”™è¯¯: {model_name}æ¨¡å‹ä¸æ”¯æŒæ—¶é—´æˆ³åŠŸèƒ½ã€‚è¯·å–æ¶ˆå‹¾é€‰'åŒ…å«æ—¶é—´æˆ³'é€‰é¡¹åé‡è¯•ã€‚"

                        results.append([
                            file.name.split("/")[-1],
                            "å¤±è´¥",
                            error_text,
                            "0s"
                        ])

                # ä¿å­˜ç»“æœåˆ°CSV
                df = pd.DataFrame(results, columns=["æ–‡ä»¶å", "çŠ¶æ€", "è¯†åˆ«ç»“æœ", "å¤„ç†æ—¶é—´"])
                csv_path = current_dir / "batch_results.csv"
                df.to_csv(csv_path, index=False, encoding='utf-8')

                yield f"æ‰¹é‡å¤„ç†å®Œæˆï¼å…±å¤„ç† {total_files} ä¸ªæ–‡ä»¶", df, gr.DownloadButton(
                    label="ğŸ“¥ ä¸‹è½½ç»“æœ(CSV)",
                    value=str(csv_path),
                    visible=True
                )

            except Exception as e:
                error_msg = f"æ‰¹é‡å¤„ç†å¤±è´¥: {str(e)}"
                yield error_msg, pd.DataFrame(), gr.DownloadButton(visible=False)

        batch_btn.click(
            batch_process,
            inputs=[file_upload, batch_model, batch_options],
            outputs=[batch_progress, batch_results, download_btn]
        )

    def create_model_management_interface(self):
        """åˆ›å»ºæ¨¡å‹ç®¡ç†ç•Œé¢"""
        with gr.Column():
            gr.Markdown("## ğŸ—‚ï¸ æ¨¡å‹ç®¡ç†")
            gr.Markdown("æŸ¥çœ‹å’Œç®¡ç†å·²ä¸‹è½½çš„æ¨¡å‹")

            with gr.Row():
                with gr.Column(scale=1):
                    refresh_btn = gr.Button("ğŸ”„ åˆ·æ–°æ¨¡å‹åˆ—è¡¨", variant="secondary")
                    clear_cache_btn = gr.Button("ğŸ—‘ï¸ æ¸…ç†ç¼“å­˜", variant="stop")

                with gr.Column(scale=3):
                    model_info = gr.Dataframe(
                        headers=["æ¨¡å‹åç§°", "ç±»å‹", "å¤§å°", "è·¯å¾„"],
                        label="å·²ä¸‹è½½æ¨¡å‹"
                    )

                    cache_info = gr.Textbox(
                        label="ç¼“å­˜ä¿¡æ¯",
                        lines=4
                    )

        def refresh_models():
            try:
                models_dir = current_dir / "models"
                model_data = []
                total_size = 0

                # æ‰«æmodelscopeç›®å½•
                ms_dir = models_dir / "modelscope"
                if ms_dir.exists():
                    for org_dir in ms_dir.iterdir():
                        if org_dir.is_dir() and not org_dir.name.startswith('.'):
                            for model_dir in org_dir.iterdir():
                                if model_dir.is_dir() and not model_dir.name.startswith('.'):
                                    size = sum(f.stat().st_size for f in model_dir.rglob('*') if f.is_file())
                                    size_mb = size / (1024 * 1024)
                                    total_size += size

                                    model_data.append([
                                        f"{org_dir.name}/{model_dir.name}",
                                        "ModelScope",
                                        f"{size_mb:.1f} MB",
                                        str(model_dir)
                                    ])

                # æ‰«æhuggingfaceç›®å½•
                hf_dir = models_dir / "huggingface"
                if hf_dir.exists():
                    hub_dir = hf_dir / "hub"
                    if hub_dir.exists():
                        for model_dir in hub_dir.iterdir():
                            if model_dir.is_dir() and not model_dir.name.startswith('.'):
                                size = sum(f.stat().st_size for f in model_dir.rglob('*') if f.is_file())
                                size_mb = size / (1024 * 1024)
                                total_size += size

                                model_data.append([
                                    model_dir.name,
                                    "HuggingFace",
                                    f"{size_mb:.1f} MB",
                                    str(model_dir)
                                ])

                total_size_gb = total_size / (1024 * 1024 * 1024)
                cache_summary = f"""
ç¼“å­˜ç»Ÿè®¡:
- æ€»æ¨¡å‹æ•°é‡: {len(model_data)}
- æ€»å ç”¨ç©ºé—´: {total_size_gb:.2f} GB
- ModelScopeç¼“å­˜: {ms_dir}
- HuggingFaceç¼“å­˜: {hf_dir}
- å·²åŠ è½½æ¨¡å‹: {len(LOADED_MODELS)}
                """.strip()

                return pd.DataFrame(model_data), cache_summary

            except Exception as e:
                return pd.DataFrame(), f"è·å–æ¨¡å‹ä¿¡æ¯å¤±è´¥: {str(e)}"

        def clear_cache():
            try:
                # æ¸…ç†å†…å­˜ä¸­çš„æ¨¡å‹
                global LOADED_MODELS
                LOADED_MODELS.clear()

                return pd.DataFrame(), "ç¼“å­˜å·²æ¸…ç†ï¼Œè¯·åˆ·æ–°æŸ¥çœ‹æœ€æ–°çŠ¶æ€"
            except Exception as e:
                return pd.DataFrame(), f"æ¸…ç†ç¼“å­˜å¤±è´¥: {str(e)}"

        refresh_btn.click(
            refresh_models,
            outputs=[model_info, cache_info]
        )

        clear_cache_btn.click(
            clear_cache,
            outputs=[model_info, cache_info]
        )

        # é¡µé¢åŠ è½½æ—¶è‡ªåŠ¨åˆ·æ–°
        refresh_models()

    def create_settings_interface(self):
        """åˆ›å»ºè®¾ç½®ç•Œé¢"""
        with gr.Column():
            gr.Markdown("## âš™ï¸ ç³»ç»Ÿè®¾ç½®")
            gr.Markdown("é…ç½®ç³»ç»Ÿå‚æ•°å’Œç¯å¢ƒè®¾ç½®")

            with gr.Row():
                with gr.Column():
                    gr.Markdown("### ç¯å¢ƒå˜é‡")

                    hf_endpoint = gr.Textbox(
                        label="HuggingFaceé•œåƒæº",
                        value=os.environ.get("HF_ENDPOINT", "https://hf-mirror.com"),
                        placeholder="https://hf-mirror.com"
                    )

                    device_setting = gr.Radio(
                        choices=["auto", "cpu", "cuda"],
                        value="auto",
                        label="è®¡ç®—è®¾å¤‡"
                    )

                    max_workers = gr.Slider(
                        1, 8, value=4, step=1,
                        label="æœ€å¤§å¹¶è¡Œæ•°"
                    )

                    apply_settings_btn = gr.Button("âœ… åº”ç”¨è®¾ç½®", variant="primary")

                with gr.Column():
                    gr.Markdown("### ç³»ç»Ÿä¿¡æ¯")

                    system_info = gr.Textbox(
                        label="ç³»ç»ŸçŠ¶æ€",
                        lines=8,
                        value=self.get_system_info()
                    )

                    refresh_info_btn = gr.Button("ğŸ”„ åˆ·æ–°ä¿¡æ¯", variant="secondary")

        def apply_settings(hf_endpoint_val, device_val, workers_val):
            try:
                # æ›´æ–°ç¯å¢ƒå˜é‡
                os.environ["HF_ENDPOINT"] = hf_endpoint_val

                # è¿™é‡Œå¯ä»¥æ·»åŠ æ›´å¤šè®¾ç½®åº”ç”¨é€»è¾‘

                return "è®¾ç½®å·²åº”ç”¨ï¼"
            except Exception as e:
                return f"åº”ç”¨è®¾ç½®å¤±è´¥: {str(e)}"

        apply_settings_btn.click(
            apply_settings,
            inputs=[hf_endpoint, device_setting, max_workers],
            outputs=[system_info]
        )

        refresh_info_btn.click(
            lambda: self.get_system_info(),
            outputs=[system_info]
        )

    def get_system_info(self) -> str:
        """è·å–ç³»ç»Ÿä¿¡æ¯"""
        try:
            import torch
            import platform

            info = []
            info.append(f"æ“ä½œç³»ç»Ÿ: {platform.system()} {platform.release()}")
            info.append(f"Pythonç‰ˆæœ¬: {platform.python_version()}")
            info.append(f"PyTorchç‰ˆæœ¬: {torch.__version__}")

            # GPUä¿¡æ¯
            if torch.cuda.is_available():
                info.append(f"CUDAå¯ç”¨: æ˜¯ (ç‰ˆæœ¬: {torch.version.cuda})")
                info.append(f"GPUæ•°é‡: {torch.cuda.device_count()}")
                for i in range(torch.cuda.device_count()):
                    gpu_name = torch.cuda.get_device_name(i)
                    gpu_memory = torch.cuda.get_device_properties(i).total_memory / 1024**3
                    info.append(f"GPU {i}: {gpu_name} ({gpu_memory:.1f} GB)")
            else:
                info.append("CUDAå¯ç”¨: å¦")

            # ç¯å¢ƒå˜é‡
            info.append("\nç¯å¢ƒå˜é‡:")
            info.append(f"HF_ENDPOINT: {os.environ.get('HF_ENDPOINT', 'æœªè®¾ç½®')}")
            info.append(f"HF_HOME: {os.environ.get('HF_HOME', 'æœªè®¾ç½®')}")
            info.append(f"MODELSCOPE_CACHE: {os.environ.get('MODELSCOPE_CACHE', 'æœªè®¾ç½®')}")

            # æ¨¡å‹ç¼“å­˜ä¿¡æ¯
            models_dir = current_dir / "models"
            if models_dir.exists():
                total_size = sum(f.stat().st_size for f in models_dir.rglob('*') if f.is_file())
                info.append(f"\næ¨¡å‹ç¼“å­˜å¤§å°: {total_size / 1024**3:.2f} GB")

            return "\n".join(info)

        except Exception as e:
            return f"è·å–ç³»ç»Ÿä¿¡æ¯å¤±è´¥: {str(e)}"

    def create_interface(self):
        """åˆ›å»ºä¸»ç•Œé¢"""
        with gr.Blocks(
                title="FunASR WebUI - æ™ºèƒ½è¯­éŸ³è¯†åˆ«å¹³å°",
                theme=gr.themes.Soft(),
                css="""
            /* å“åº”å¼å®¹å™¨è®¾ç½® */
            .gradio-container {
                max-width: 95vw !important;
                min-width: 800px !important;
                width: 100% !important;
                margin: 0 auto !important;
                padding: 10px !important;
            }

            /* ä¸»æ ‡é¢˜æ ·å¼ */
            .main-header {
                text-align: center;
                background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
                color: white;
                padding: 20px;
                border-radius: 10px;
                margin-bottom: 20px;
                box-shadow: 0 4px 15px rgba(0,0,0,0.2);
            }

            /* å“åº”å¼æ ‡ç­¾é¡µ */
            .gradio-tabs {
                width: 100% !important;
            }

            /* è¾“å…¥è¾“å‡ºåŒºåŸŸè‡ªé€‚åº” */
            .gradio-row {
                width: 100% !important;
                display: flex !important;
                flex-wrap: wrap !important;
                gap: 15px !important;
            }

            .gradio-column {
                flex: 1 !important;
                min-width: 300px !important;
            }

            /* éŸ³é¢‘ç»„ä»¶ä¼˜åŒ– */
            .gradio-audio {
                width: 100% !important;
                max-width: none !important;
            }

            /* æŒ‰é’®æ ·å¼ä¼˜åŒ– */
            .gradio-button {
                width: 100% !important;
                min-height: 45px !important;
                font-size: 16px !important;
                margin: 5px 0 !important;
            }

            /* æ–‡æœ¬æ¡†è‡ªé€‚åº” */
            .gradio-textbox {
                width: 100% !important;
            }

            /* ä¸‹æ‹‰æ¡†ä¼˜åŒ– */
            .gradio-dropdown {
                width: 100% !important;
            }

            /* å¤§å±å¹•ä¼˜åŒ– */
            @media (min-width: 1200px) {
                .gradio-container {
                    max-width: 1400px !important;
                }
                .gradio-column {
                    min-width: 400px !important;
                }
            }

            /* ä¸­ç­‰å±å¹•ä¼˜åŒ– */
            @media (max-width: 1024px) {
                .gradio-container {
                    max-width: 95vw !important;
                    padding: 8px !important;
                }
                .gradio-column {
                    min-width: 280px !important;
                }
                .main-header {
                    padding: 15px;
                }
            }

            /* å°å±å¹•ä¼˜åŒ– */
            @media (max-width: 768px) {
                .gradio-container {
                    min-width: 100% !important;
                    padding: 5px !important;
                }
                .gradio-row {
                    flex-direction: column !important;
                }
                .gradio-column {
                    min-width: 100% !important;
                    width: 100% !important;
                }
                .main-header h1 {
                    font-size: 1.8em !important;
                }
                .main-header p {
                    font-size: 0.9em !important;
                }
            }

            /* è¶…å¤§å±å¹•ä¼˜åŒ– */
            @media (min-width: 1600px) {
                .gradio-container {
                    max-width: 1600px !important;
                }
                .gradio-column {
                    min-width: 500px !important;
                }
            }

            /* é«˜åº¦è‡ªé€‚åº” */
            .gradio-interface {
                min-height: 100vh !important;
            }

            /* è¡¨æ ¼å“åº”å¼ */
            .gradio-dataframe {
                width: 100% !important;
                overflow-x: auto !important;
            }

            /* JSONæ˜¾ç¤ºä¼˜åŒ– */
            .gradio-json {
                width: 100% !important;
                max-height: 400px !important;
                overflow-y: auto !important;
            }

            /* æ–‡ä»¶ä¸Šä¼ ç»„ä»¶ä¼˜åŒ– */
            .gradio-file {
                width: 100% !important;
            }

            /* æ»‘å—ç»„ä»¶ä¼˜åŒ– */
            .gradio-slider {
                width: 100% !important;
            }

            /* å¤é€‰æ¡†ç»„ä¼˜åŒ– */
            .gradio-checkboxgroup, .gradio-checkbox {
                width: 100% !important;
            }

            /* æ ‡ç­¾é¡µå†…å®¹ä¼˜åŒ– */
            .gradio-tab-nav {
                flex-wrap: wrap !important;
            }

            /* æ‰‹æœºç«¯æ ‡ç­¾é¡µä¼˜åŒ– */
            @media (max-width: 480px) {
                .gradio-tab-nav button {
                    font-size: 12px !important;
                    padding: 8px 12px !important;
                }
            }
            """
        ) as interface:

            gr.HTML("""
            <div class="main-header">
                <h1>ğŸ¤ FunASR è¯­éŸ³è¯†åˆ«</h1>
                <p>æ™ºèƒ½è¯­éŸ³è¯†åˆ«å¹³å° - æ”¯æŒå¤šè¯­è¨€ã€å¤šæ¨¡å‹ã€å¤šåŠŸèƒ½çš„è¯­éŸ³å¤„ç†</p>
            </div>
            """)

            with gr.Tabs() as tabs:
                with gr.TabItem("ğŸ¤ åŸºç¡€è¯†åˆ«", id="basic"):
                    self.create_basic_asr_interface()

                with gr.TabItem("âš™ï¸ é«˜çº§åŠŸèƒ½", id="advanced"):
                    self.create_advanced_interface()

                with gr.TabItem("ğŸ“ æ‰¹é‡å¤„ç†", id="batch"):
                    self.create_batch_interface()

                with gr.TabItem("ğŸ—‚ï¸ æ¨¡å‹ç®¡ç†", id="models"):
                    self.create_model_management_interface()

                with gr.TabItem("âš™ï¸ ç³»ç»Ÿè®¾ç½®", id="settings"):
                    self.create_settings_interface()

            gr.Markdown("""
            ---
            ### ğŸ“– ä½¿ç”¨è¯´æ˜
            - **åŸºç¡€è¯†åˆ«**: ç®€å•å¿«é€Ÿçš„è¯­éŸ³è¯†åˆ«ï¼Œæ”¯æŒæ–‡ä»¶ä¸Šä¼ å’Œå®æ—¶å½•éŸ³
            - **é«˜çº§åŠŸèƒ½**: åŒ…å«VADã€æ ‡ç‚¹æ¢å¤ã€è¯´è¯äººåˆ†ç¦»ã€æ—¶é—´æˆ³ç­‰åŠŸèƒ½
            - **æ‰¹é‡å¤„ç†**: ä¸€æ¬¡æ€§å¤„ç†å¤šä¸ªéŸ³é¢‘æ–‡ä»¶
            - **æ¨¡å‹ç®¡ç†**: æŸ¥çœ‹å·²ä¸‹è½½æ¨¡å‹ï¼Œç®¡ç†ç¼“å­˜ç©ºé—´
            - **ç³»ç»Ÿè®¾ç½®**: é…ç½®ç¯å¢ƒå‚æ•°ï¼ŒæŸ¥çœ‹ç³»ç»ŸçŠ¶æ€

            ğŸ’¡ **æç¤º**: é¦–æ¬¡ä½¿ç”¨éœ€è¦ä¸‹è½½æ¨¡å‹ï¼Œè¯·è€å¿ƒç­‰å¾…
            """)

        return interface

def main():
    """ä¸»å‡½æ•°"""
    import argparse

    parser = argparse.ArgumentParser(description="FunASR WebUI")
    parser.add_argument("--host", default="0.0.0.0", help="æœåŠ¡å™¨åœ°å€")
    parser.add_argument("--port", type=int, default=7860, help="ç«¯å£å·")
    parser.add_argument("--share", action="store_true", help="å¯ç”¨Gradioå…¬å…±åˆ†äº«")
    parser.add_argument("--debug", action="store_true", help="å¯ç”¨è°ƒè¯•æ¨¡å¼")
    parser.add_argument("--auth", nargs=2, metavar=("USERNAME", "PASSWORD"), help="è®¾ç½®ç™»å½•è®¤è¯")

    args = parser.parse_args()

    webui = FunASRWebUI()
    interface = webui.create_interface()

    # è®¾ç½®è®¤è¯
    auth = tuple(args.auth) if args.auth else None

    print(f"ğŸ¤ FunASR WebUI å¯åŠ¨ä¸­...")
    print(f"ğŸ“ è®¿é—®åœ°å€: http://{args.host}:{args.port}")
    if args.share:
        print(f"ğŸŒ å…¬å…±åˆ†äº«å·²å¯ç”¨")
    if auth:
        print(f"ğŸ” å·²å¯ç”¨ç™»å½•è®¤è¯")

    interface.launch(
        server_name=args.host,
        server_port=args.port,
        share=args.share,
        debug=args.debug,
        show_api=False,
        auth=auth,
        favicon_path=None,
        app_kwargs={"title": "FunASR WebUI"}
    )

if __name__ == "__main__":
    main()

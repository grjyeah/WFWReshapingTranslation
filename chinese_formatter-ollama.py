import requests
import json
import re
from typing import List, Dict, Tuple
from difflib import SequenceMatcher


class ChineseFormatter:
    """ä¸­æ–‡ä¼šè®®é€å­—ç¨¿ä¹¦é¢åŒ–å¤„ç†"""

    def __init__(self, ollama_url: str = "http://localhost:11434",
                 # model_name: str = "yasserrmd/Qwen2.5-7B-Instruct-1M:latest"):
                 model_name: str = "did100/qwen2.5-32B-Instruct-Q4_K_M:latest"):
                 # model_name: str = "alibayram/Qwen3-30B-A3B-Instruct-2507:latest"):
        self.ollama_url = ollama_url
        self.model_name = model_name
        self.api_endpoint = f"{ollama_url}/api/generate"

        # æ¨¡å‹å‚æ•°é…ç½®ï¼ˆä¼˜åŒ–ä¸ºç²¾ç®€ä¹¦é¢åŒ–è¾“å‡ºï¼‰
        self.model_options = {
            # "mirostat": 2,
            # "mirostat_tau": 5.0,  # ä¸­æ–‡è¿è´¯æ€§æœ€ä½³åŒºé—´
            # "mirostat_eta": 0.1,
            "repeat_penalty": 1.15,
            "num_thread": 8,  # GPU offload åï¼ŒCPU åªéœ€å¤„ç†å‰©ä½™å±‚ï¼Œ8 çº¿ç¨‹è¶³å¤Ÿ
            "num_batch": 512,  # é»˜è®¤å³å¯ï¼Œæˆ–è®¾ä¸º 1024 æå‡åå
            "rope_frequency_base": 1000000,   # Qwen é•¿æ–‡æœ¬é€‚é…

            "num_ctx": 131072,  # ä¸Šä¸‹æ–‡çª—å£å¤§å°
            "num_predict": 8192,  # é™åˆ¶æœ€å¤§è¾“å‡ºï¼Œé˜²æ­¢è¿‡åº¦å†—é•¿
            "temperature": 0.3,  # é™ä½æ¸©åº¦ï¼Œä½¿è¾“å‡ºæ›´ç®€æ´è§„èŒƒ
            "top_p": 0.85,  # é™ä½top-pï¼Œå‡å°‘å‘æ•£
            "top_k": 30,  # é™ä½top-kï¼Œæ›´èšç„¦
            "repeat_penalty": 1.15,  # æé«˜é‡å¤æƒ©ç½šï¼Œé¿å…å•°å—¦
            "presence_penalty": 0.2,  # æé«˜å­˜åœ¨æƒ©ç½š
            "frequency_penalty": 0.2,  # æé«˜é¢‘ç‡æƒ©ç½š
            "stop": ["\n\n\n", "============", "End of", "ã€ç»“æŸã€‘"]  # æ·»åŠ åœæ­¢è¯
        }

        # æç¤ºè¯æ¨¡æ¿
        self.processing_prompt = """<instructions>
    <role>
        ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„è¯­è¨€ç¼–è¾‘ï¼Œæ“…é•¿å°†å£è¯­åŒ–çš„ä¼šè®®é€å­—ç¨¿è½¬æ¢ä¸ºæ­£å¼çš„ä¹¦é¢è¯­æ–‡æ¡£ã€‚
    </role>

    <task>
        <requirement type="core" priority="highest">
            <title>æ ¸å¿ƒè¦æ±‚ - é€å¥è½¬æ¢ï¼Œä¸åšæ€»ç»“</title>
            <item id="1">
                <name>é€å¥è½¬æ¢</name>
                <rules>
                    <rule>å¯¹åŸæ–‡ä¸­çš„æ¯ä¸€å¥è¯è¿›è¡Œä¹¦é¢åŒ–æ”¹å†™ï¼Œä¸è¦æ— ä¸­ç”Ÿæœ‰</rule>
                    <rule>ä¸¥ç¦æ€»ç»“ã€æ¦‚æ‹¬æˆ–å½’çº³</rule>
                    <rule>è¾“å…¥æ–‡æœ¬æ¯å¥è¯æ¢ä¸€è¡Œï¼Œä½†è¾“å‡ºéœ€æ ¹æ®è¯´è¯äººæ ‡ç­¾æŠŠåŒä¸€ä¸ªäººè¯´çš„è¯åˆå¹¶åˆ°ä¸€ä¸ªæ®µè½ï¼Œä¸è¦å•å¥åˆ†è¡Œ</rule>
                    <rule>ä¿ç•™æ‰€æœ‰è¯´è¯äººçš„æ‰€æœ‰å‘è¨€å†…å®¹</rule>
                    <rule>æ¯å¥è¯å¼€å§‹æœ‰å¯¹åº”çš„è¯´è¯äººæ ‡ç­¾ï¼Œä¾‹å¦‚ã€è¯´è¯äºº:0ã€‘ã€ã€è¯´è¯äºº:1ã€‘ç­‰</rule>
                    <rule>æ ¹æ®æ ‡ç­¾åŒºåˆ†åŒä¸€ä¸ªè¯´è¯äººçš„å†…å®¹ï¼Œå¦‚æœæ˜¯åŒä¸€è¯´è¯äººï¼ŒæŠŠå†…å®¹åˆå¹¶æ”¾åˆ°ä¸€ä¸ªæ®µè½ä¸­ï¼Œä¸éœ€è¦å¤ªå¤šæ¢è¡Œ</rule>
                    <rule condition="empty_content">å¦‚æœæ•´å¥è¯åˆ¤æ–­ä¸ºçº¯å£è¯­æ²¡æœ‰è¾“å‡ºï¼Œè¿™è¡Œä¸éœ€è¦è¾“å‡ºï¼Œä¸ç”¨è¾“å‡ºç©ºç™½çš„è¡Œ</rule>
                    <rule condition="empty_content">ä¾‹å¦‚ã€è¯´è¯äºº:Xã€‘ï¼š(æ ‡ç­¾ååªæœ‰æ¢è¡Œç¬¦)ã€è¯´è¯äºº:Xã€‘ï¼š(æ ‡ç­¾åæ²¡å†…å®¹)çš„ï¼Œè¿æ ‡ç­¾ä¹Ÿä¸éœ€è¦è¾“å‡º</rule>
                </rules>
            </item>

            <item id="2">
                <name>ä¹¦é¢åŒ–æ”¹å†™</name>
                <rules>
                    <rule>åˆ é™¤æ‰€æœ‰å£è¯­è¯ï¼š"é‚£ä¸ª"ã€"ç„¶å"ã€"å°±æ˜¯è¯´"ã€"å‘ƒ"ã€"å—¯"ã€"å•Š"ã€"å‘¢"ã€"å“‡"ç­‰</rule>
                    <rule>ä¿ç•™æ‰€æœ‰å®è´¨æ€§å†…å®¹ã€æ•°æ®ã€è§‚ç‚¹ã€è®¨è®ºç»†èŠ‚</rule>
                    <rule>å°†å£è¯­è¡¨è¾¾æ”¹ä¸ºæ­£å¼ä¹¦é¢è¯­è¡¨è¾¾</rule>
                    <rule>æ¶¦è‰²è¯­è¨€ï¼Œä½¿è¡¨è¾¾æ›´ä¸“ä¸šã€æ›´è§„èŒƒ</rule>
                </rules>
            </item>

            <item id="3" type="prohibition">
                <name>ä¸¥ç¦ä»¥ä¸‹è¡Œä¸º</name>
                <prohibitions>
                    <prohibition>ä¸¥ç¦åˆ é™¤ä»»ä½•å‘è¨€å†…å®¹</prohibition>
                    <prohibition>ä¸¥ç¦æ€»ç»“æ¦‚æ‹¬ï¼ˆå¦‚"ä¸»è¦è®¨è®ºäº†"ã€"é‡ç‚¹æåˆ°"ï¼‰</prohibition>
                    <prohibition>ä¸¥ç¦åˆå¹¶å¥å­æˆ–æ®µè½</prohibition>
                    <prohibition>ä¸¥ç¦æç‚¼è¦ç‚¹</prohibition>
                    <prohibition>ä¸¥ç¦æ·»åŠ åŸæ–‡ä¸­æ²¡æœ‰çš„å†…å®¹</prohibition>
                </prohibitions>
            </item>

            <item id="4">
                <name>è¾“å‡ºæ ¼å¼è¦æ±‚</name>
                <requirements>
                    <requirement>ä¿ç•™æ‰€æœ‰è¯´è¯äººæ ‡è¯†</requirement>
                    <requirement>ä¿ç•™åŸæœ‰çš„å¯¹è¯ç»“æ„å’Œé¡ºåº</requirement>
                    <requirement>æ¯ä¸ªè¯´è¯äººçš„å‘è¨€éƒ½è¦å®Œæ•´ä¿ç•™</requirement>
                    <requirement>è¯­è¨€é£æ ¼ï¼šæ­£å¼ã€ä¸“ä¸šã€å®¢è§‚</requirement>
                </requirements>
            </item>

            <item id="5" type="prohibition">
                <name>ä¸¥ç¦è¾“å‡ºæ— å…³å†…å®¹</name>
                <prohibitions>
                    <prohibition>ä¸¥ç¦æ·»åŠ ä»»ä½•æ ‡é¢˜ï¼ˆå¦‚"### ä¼šè®®è®°å½•"ã€"ã€é€å¥ä¹¦é¢åŒ–æ”¹å†™ã€‘"ï¼‰</prohibition>
                    <prohibition>ä¸¥ç¦æ·»åŠ è¯´æ˜æ€§æ–‡å­—ï¼ˆå¦‚"ä»¥ä¸‹æ˜¯..."ã€"æ”¹å†™å¦‚ä¸‹ï¼š"ï¼‰</prohibition>
                    <prohibition>ä¸¥ç¦æ·»åŠ å‰è¨€ã€åè¯­ã€æ€»ç»“æ€§æ–‡å­—</prohibition>
                    <prohibition priority="critical">åªè¾“å‡ºå¯¹è¯æœ¬èº«ï¼Œä»ç¬¬ä¸€ä¸ªè¯´è¯äººå¼€å§‹ï¼Œåˆ°æœ€åä¸€ä¸ªè¯´è¯äººç»“æŸ</prohibition>
                    <prohibition priority="critical">ä¸è¦æœ‰ä»»ä½•å…¶ä»–æ–‡å­—ï¼Œåªè¦å¯¹è¯</prohibition>
                </prohibitions>
            </item>

            <item id="6">
                <name>æ ¼å¼è¦æ±‚</name>
                <formatting_rules>
                    <rule id="speaker_label">
                        <name>è¯´è¯äººæ ‡è¯†æ ¼å¼ç»Ÿä¸€</name>
                        <description>ä½¿ç”¨ã€è¯´è¯äººå§“åã€‘ï¼ˆä¹¦åå·ï¼‰ï¼Œä¸è¦ç”¨æ–¹æ‹¬å·[]</description>
                    </rule>
                    <rule id="paragraph_organization">
                        <name>æ®µè½ç»„ç»‡</name>
                        <description>åŒä¸€ä¸ªè¯´è¯äººçš„è¿ç»­å‘è¨€ï¼Œåˆå¹¶ä¸ºä¸€ä¸ªå®Œæ•´çš„è¯­ä¹‰æ®µè½</description>
                    </rule>
                    <rule id="line_break">
                        <name>æ¢è¡Œè§„åˆ™</name>
                        <description>æ¯ä¸ªè¯´è¯äººçš„å®Œæ•´è¯­ä¹‰æ®µè½ç»“æŸåï¼Œå¿…é¡»æ¢è¡Œï¼ˆæ¯ä¸ªè¯´è¯äººå ä¸€è¡Œï¼‰</description>
                    </rule>
                    <rule id="sentence_continuity">
                        <name>ä¸è¦å¤šå¥è¯æ¢è¡Œ</name>
                        <description>å°†åŒä¸€è¯´è¯äººçš„ç›¸å…³å¥å­ç»„ç»‡æˆè¿è´¯çš„æ®µè½</description>
                    </rule>
                    <rule id="one_speaker_one_paragraph">
                        <name>ä¸€ä¸ªè¯´è¯äºº=ä¸€ä¸ªæ®µè½</name>
                        <format>ã€è¯´è¯äººã€‘ï¼šå®Œæ•´å†…å®¹ï¼ˆå¯åŒ…å«å¤šä¸ªå¥å­ï¼‰ç„¶åæ¢è¡Œ</format>
                    </rule>
                </formatting_rules>
            </item>

            <item id="7">
                <name>è¾“å‡ºç¤ºä¾‹</name>
                <examples>
                    <example type="correct">
                        <description>æ­£ç¡®æ ¼å¼</description>
                        <content>
                            ã€ä¸»æŒäººã€‘ï¼šå¤§å®¶å¥½ï¼Œæ¬¢è¿å‚åŠ ä»Šå¤©çš„ä¼šè®®ã€‚ä»Šå¤©æˆ‘ä»¬ä¸»è¦è®¨è®ºæ•°æ®æ²»ç†çš„ç›¸å…³å·¥ä½œã€‚
                            
                            ã€å¼ æ€»ã€‘ï¼šæˆ‘ä»¬éœ€è¦å»ºç«‹ä¸€ä¸ªå®Œå–„çš„æ•°æ®æ²»ç†ä½“ç³»ã€‚æ•°æ®æ²»ç†éå¸¸é‡è¦ï¼Œå¯¹ä¼ä¸šçš„æ•°å­—åŒ–è½¬å‹è‡³å…³é‡è¦ã€‚
                            
                            ã€æç»ç†ã€‘ï¼šæˆ‘åŒæ„å¼ æ€»çš„è¯´æ³•ã€‚æˆ‘ä»¬çš„å¹³å°èƒ½å¤Ÿæä¾›æœ‰æ•ˆçš„æ”¯æŒã€‚
                        </content>
                    </example>
                    <example type="incorrect">
                        <description>é”™è¯¯æ ¼å¼</description>
                        <errors>
                            <error>ã€ä¸»æŒäººã€‘ï¼šå¤§å®¶å¥½ï¼Œã€å¼ æ€»ã€‘ï¼šæˆ‘ä»¬éœ€è¦å»ºç«‹...ï¼ˆä¸è¦æŠŠä¸åŒè¯´è¯äººæ”¾åœ¨ä¸€èµ·ï¼‰</error>
                            <error>ã€ä¸»æŒäººã€‘ï¼šå¤§å®¶å¥½ã€‚ä»Šå¤©æˆ‘ä»¬è®¨è®ºã€‚ç›¸å…³çš„å·¥ä½œã€‚ï¼ˆåŒä¸€è¯´è¯äººçš„ç›¸å…³å¥å­ä¸è¦æ–­å¼€ï¼‰</error>
                        </errors>
                    </example>
                </examples>
                <core_principle>é€å¥ä¹¦é¢åŒ–æ”¹å†™ï¼Œä¸åˆ ä¸å‡ï¼Œä¿ç•™è¯´è¯äººï¼Œä¸€ä¸ªè¯´è¯äººä¸€ä¸ªæ®µè½ï¼</core_principle>
            </item>
        </requirement>
    </task>

    <input>
        <metadata>
            <original_text_length unit="characters">{text_length}</original_text_length>
        </metadata>
        <content>
            <![CDATA[{text}]]>
        </content>
    </input>

    <output_requirement>
        <target_length unit="characters">
            <value>{target_length}</value>
            <tolerance>Â±10%</tolerance>
        </target_length>
        <format>ä¸¥æ ¼æŒ‰ç…§ä¸Šè¿°ç¤ºä¾‹æ ¼å¼ï¼Œç›´æ¥è¾“å‡ºé€å¥ä¹¦é¢åŒ–æ”¹å†™åçš„å¯¹è¯</format>
    </output_requirement>
</instructions>"""

    def split_text(self, text: str, max_chars: int = 1000) -> List[str]:
        """
        å°†é•¿æ–‡æœ¬æŒ‰æ®µè½æ™ºèƒ½åˆ†å‰²ï¼Œç¡®ä¿åœ¨å¥å­è¾¹ç•Œåˆ‡åˆ†

        Args:
            text: åŸå§‹æ–‡æœ¬
            max_chars: æ¯æ®µæœ€å¤§å­—ç¬¦æ•°

        Returns:
            åˆ†å‰²åçš„æ–‡æœ¬ç‰‡æ®µåˆ—è¡¨
        """
        # æŒ‰è¯´è¯äººå’Œå¥å­åˆ†å‰²ï¼Œä¿æŒè¯­ä¹‰å®Œæ•´æ€§
        speaker_pattern = r'\[([^\]]+)\]ï¼š'
        sentence_endings = r'[ã€‚ï¼ï¼Ÿï¼›â€¦\n]+'

        # å…ˆæå–æ‰€æœ‰è¯´è¯äººæ®µè½
        speaker_blocks = []
        current_speaker = None
        current_content = []

        lines = text.split('\n')
        for line in lines:
            speaker_match = re.match(speaker_pattern, line)
            if speaker_match:
                # ä¿å­˜å‰ä¸€ä¸ªè¯´è¯äººçš„å†…å®¹
                if current_speaker and current_content:
                    content = ''.join(current_content).strip()
                    if content:
                        speaker_blocks.append(f"[{current_speaker}]ï¼š{content}")
                # å¼€å§‹æ–°çš„è¯´è¯äºº
                current_speaker = speaker_match.group(1)
                current_content = [line[len(speaker_match.group(0)):]]
            elif current_speaker:
                current_content.append(line)

        # ä¿å­˜æœ€åä¸€ä¸ªè¯´è¯äºº
        if current_speaker and current_content:
            content = ''.join(current_content).strip()
            if content:
                speaker_blocks.append(f"[{current_speaker}]ï¼š{content}")

        # å¦‚æœæ²¡æœ‰æ£€æµ‹åˆ°è¯´è¯äººæ ¼å¼ï¼ŒæŒ‰å¥å­åˆ†å‰²
        if not speaker_blocks:
            sentences = re.split(f'({sentence_endings})', text)
            chunks = []
            current_chunk = ""

            for i in range(0, len(sentences), 2):
                sentence = sentences[i]
                # æ·»åŠ æ ‡ç‚¹ç¬¦å·
                if i + 1 < len(sentences):
                    sentence += sentences[i + 1]

                if len(current_chunk) + len(sentence) <= max_chars:
                    current_chunk += sentence
                else:
                    if current_chunk.strip():
                        chunks.append(current_chunk.strip())
                    current_chunk = sentence

            if current_chunk.strip():
                chunks.append(current_chunk.strip())
            return chunks if chunks else [text]

        # æŒ‰è¯´è¯äººå—ç»„åˆæˆchunks
        chunks = []
        current_chunk = ""

        for block in speaker_blocks:
            if len(current_chunk) + len(block) <= max_chars:
                current_chunk += ("\n\n" if current_chunk else "") + block
            else:
                if current_chunk.strip():
                    chunks.append(current_chunk.strip())
                current_chunk = block

        if current_chunk.strip():
            chunks.append(current_chunk.strip())

        return chunks if chunks else [text]

    def remove_duplicates(self, text: str) -> str:
        """
        ä½¿ç”¨ä»£ç å®ç°å»é‡ï¼Œåˆ é™¤é‡å¤çš„å¥å­å’Œæ®µè½
        é˜²æ­¢æ¨¡å‹å¹»è§‰ï¼Œç¡®ä¿å†…å®¹ç®€æ´

        Args:
            text: å¾…å»é‡çš„æ–‡æœ¬

        Returns:
            å»é‡åçš„æ–‡æœ¬
        """
        # æŒ‰å¥å­åˆ†å‰²ï¼ˆåªåœ¨å¥å­ç»“æŸç¬¦å·å¤„åˆ†å‰²ï¼Œä¿ç•™æ¢è¡Œï¼‰
        sentence_ends = []
        for i, char in enumerate(text):
            if char in 'ã€‚ï¼ï¼Ÿï¼›':
                sentence_ends.append(i)

        # æå–å¥å­ï¼ˆä¿ç•™æ¢è¡Œç¬¦ï¼‰
        sentence_list = []
        last_end = -1
        for end in sentence_ends:
            sentence = text[last_end + 1:end + 1].strip()
            if sentence:
                sentence_list.append(sentence)
            last_end = end

        # å¤„ç†å‰©ä½™æ–‡æœ¬ï¼ˆå¦‚æœæ²¡æœ‰å¥å­ç»“æŸç¬¦ï¼‰
        if last_end < len(text) - 1:
            remaining = text[last_end + 1:].strip()
            if remaining:
                sentence_list.append(remaining)

        # å»é‡é€»è¾‘ï¼šä½¿ç”¨ç›¸ä¼¼åº¦æ£€æµ‹
        unique_sentences = []
        similarity_threshold = 0.80  # ç›¸ä¼¼åº¦é˜ˆå€¼ï¼ˆé™ä½ä»¥æ£€æµ‹æ›´å¤šé‡å¤ï¼‰

        for sentence in sentence_list:
            is_duplicate = False

            # ä¸å·²ä¿ç•™çš„å¥å­è¿›è¡Œæ¯”è¾ƒ
            for kept_sentence in unique_sentences:
                # ç§»é™¤æ ‡ç‚¹ç¬¦å·å’Œç©ºæ ¼è¿›è¡Œæ¯”è¾ƒ
                s1_clean = re.sub(r'[ã€‚ï¼ï¼Ÿï¼›ï¼Œã€\s]', '', sentence)
                s2_clean = re.sub(r'[ã€‚ï¼ï¼Ÿï¼›ï¼Œã€\s]', '', kept_sentence)

                if not s1_clean or not s2_clean:
                    continue

                similarity = SequenceMatcher(None, s1_clean, s2_clean).ratio()

                # å¦‚æœç›¸ä¼¼åº¦è¶…è¿‡é˜ˆå€¼ï¼Œè®¤ä¸ºæ˜¯é‡å¤
                if similarity >= similarity_threshold:
                    is_duplicate = True
                    # ä¿ç•™è¾ƒé•¿çš„å¥å­ï¼ˆé€šå¸¸æ›´å®Œæ•´ï¼‰
                    if len(sentence) > len(kept_sentence):
                        unique_sentences.remove(kept_sentence)
                        unique_sentences.append(sentence)
                    break

            if not is_duplicate:
                unique_sentences.append(sentence)

        # åˆå¹¶å»é‡åçš„å¥å­
        dedup_text = ''.join(unique_sentences)

        # æ®µè½çº§åˆ«å»é‡ï¼šåˆ é™¤é‡å¤çš„è¯´è¯äººæ®µè½
        if '[' in dedup_text and ']ï¼š' in dedup_text:
            paragraphs = dedup_text.split('\n\n')
            unique_paragraphs = []

            for para in paragraphs:
                para = para.strip()
                if not para:
                    continue

                # æå–è¯´è¯äººæ ‡è¯†
                speaker_match = re.match(r'\[[^\]]+\]ï¼š', para)

                is_duplicate_para = False
                for kept_para in unique_paragraphs:
                    kept_match = re.match(r'\[[^\]]+\]ï¼š', kept_para)

                    # åŒä¸€ä¸ªè¯´è¯äººï¼Œæ£€æŸ¥å†…å®¹ç›¸ä¼¼åº¦
                    if speaker_match and kept_match:
                        if speaker_match.group(0) == kept_match.group(0):
                            # ç§»é™¤è¯´è¯äººæ ‡è¯†åæ¯”è¾ƒå†…å®¹
                            content1 = para[len(speaker_match.group(0)):].strip()
                            content2 = kept_para[len(kept_match.group(0)):].strip()

                            if content1 and content2:
                                # ç§»é™¤æ ‡ç‚¹ç¬¦å·æ¯”è¾ƒ
                                c1_clean = re.sub(r'[ã€‚ï¼ï¼Ÿï¼›ï¼Œã€\s]', '', content1)
                                c2_clean = re.sub(r'[ã€‚ï¼ï¼Ÿï¼›ï¼Œã€\s]', '', content2)

                                similarity = SequenceMatcher(None, c1_clean, c2_clean).ratio()
                                if similarity >= similarity_threshold:
                                    is_duplicate_para = True
                                    # ä¿ç•™è¾ƒé•¿çš„æ®µè½
                                    if len(para) > len(kept_para):
                                        unique_paragraphs.remove(kept_para)
                                        unique_paragraphs.append(para)
                                    break

                if not is_duplicate_para:
                    unique_paragraphs.append(para)

            dedup_text = '\n\n'.join(unique_paragraphs)

        return dedup_text

    def format_speaker_paragraphs(self, text: str) -> str:
        """
        æ ¼å¼åŒ–è¯´è¯äººæ®µè½ï¼šç¡®ä¿æ¯ä¸ªè¯´è¯äººçš„æ ‡è¯†åè·Ÿå®Œæ•´æ®µè½ï¼Œå¹¶æ¢è¡Œ

        Args:
            text: å¾…æ ¼å¼åŒ–çš„æ–‡æœ¬

        Returns:
            æ ¼å¼åŒ–åçš„æ–‡æœ¬
        """
        import re

        # åœ¨æ¯ä¸ªã€è¯´è¯äººã€‘å‰æ¢è¡Œï¼ˆé™¤äº†ç¬¬ä¸€ä¸ªï¼‰
        # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼åŒ¹é…è¯´è¯äººæ ‡è¯†
        pattern = r'ã€([^ã€‘]+)ã€‘'

        def replacer(match):
            # å¦‚æœåŒ¹é…çš„ä¸æ˜¯åœ¨å¼€å¤´ï¼Œå°±åœ¨å‰é¢åŠ æ¢è¡Œ
            matched_text = match.group(0)
            if match.start() > 0:
                return '\n' + matched_text
            return matched_text

        # æ›¿æ¢æ‰€æœ‰è¯´è¯äººæ ‡è¯†ï¼Œç¡®ä¿æ¯ä¸ªéƒ½åœ¨æ–°è¡Œå¼€å§‹
        formatted = re.sub(pattern, replacer, text)

        # å»é™¤å¼€å¤´çš„å¤šä½™æ¢è¡Œ
        formatted = formatted.lstrip('\n')

        return formatted

    def detect_repetition(self, text: str, window_size: int = 100) -> bool:
        """
        æ£€æµ‹æ–‡æœ¬æ˜¯å¦å‡ºç°é‡å¤å¾ªç¯

        Args:
            text: å¾…æ£€æµ‹çš„æ–‡æœ¬
            window_size: æ£€æµ‹çª—å£å¤§å°

        Returns:
            True if repetition detected
        """
        if len(text) < window_size * 2:
            return False

        # æ£€æŸ¥æœ€åwindow_sizeä¸ªå­—ç¬¦æ˜¯å¦åœ¨å‰é¢å‡ºç°è¿‡
        tail = text[-window_size:]

        # åœ¨æœ€å500ä¸ªå­—ç¬¦ä¸­æŸ¥æ‰¾é‡å¤
        search_range = text[-500:-window_size] if len(text) > 500 else text[:-window_size]

        if tail in search_range:
            return True

        return False

    def call_ollama(self, prompt: str, max_retries: int = 2, use_stream: bool = True) -> str:
        """
        è°ƒç”¨æœ¬åœ°Ollamaæ¨¡å‹ï¼Œæ”¯æŒæµå¼è¾“å‡ºå’Œé‡è¯•æœºåˆ¶

        Args:
            prompt: è¾“å…¥æç¤ºè¯
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
            use_stream: æ˜¯å¦ä½¿ç”¨æµå¼è¾“å‡ºï¼ˆé»˜è®¤Trueï¼Œé˜²æ­¢å¡æ­»ï¼‰

        Returns:
            æ¨¡å‹ç”Ÿæˆçš„æ–‡æœ¬
        """
        payload = {
            "model": self.model_name,
            "prompt": prompt,
            "stream": use_stream,
            "options": self.model_options,
            "num_gpu_layers": 60  # æ ¹æ®ä½ çš„GPUæ˜¾å­˜è°ƒæ•´æ•°å€¼
        }

        for attempt in range(max_retries + 1):
            try:
                if use_stream:
                    # ä½¿ç”¨æµå¼è¾“å‡ºï¼Œå®æ—¶ç›‘æ§ç”Ÿæˆè¿›åº¦
                    return self._stream_response(payload, attempt)
                else:
                    # éæµå¼æ¨¡å¼
                    response = requests.post(
                        self.api_endpoint,
                        json=payload,
                        timeout=180  # 3åˆ†é’Ÿè¶…æ—¶ï¼ˆä»10åˆ†é’Ÿç¼©çŸ­ï¼‰
                    )
                    response.raise_for_status()

                    result = response.json()
                    response_text = result.get('response', '').strip()

                    if response_text:
                        return response_text
                    elif attempt < max_retries:
                        print(f"  ç¬¬{attempt + 1}æ¬¡å°è¯•è¿”å›ç©ºç»“æœï¼Œé‡è¯•ä¸­...")
                    else:
                        return ""

            except requests.exceptions.Timeout as err:
                if attempt < max_retries:
                    print(f"\n  â±ï¸ ç¬¬{attempt + 1}æ¬¡å°è¯•è¶…æ—¶ï¼ˆ3åˆ†é’Ÿï¼‰ï¼Œé‡è¯•ä¸­...")
                else:
                    print(f"\n  âŒ APIè°ƒç”¨è¶…æ—¶: {err}")
                    return ""

            except requests.exceptions.RequestException as err:
                if attempt < max_retries:
                    print(f"\n  ğŸ”„ ç¬¬{attempt + 1}æ¬¡å°è¯•å‡ºé”™: {err}ï¼Œé‡è¯•ä¸­...")
                else:
                    print(f"\n  âŒ APIè°ƒç”¨é”™è¯¯: {err}")
                    return ""

        return ""

    def _stream_response(self, payload: dict, attempt: int) -> str:
        """
        æµå¼å“åº”å¤„ç†ï¼Œæ™ºèƒ½æ£€æµ‹é‡å¤å¹¶åœæ­¢

        Args:
            payload: è¯·æ±‚payload
            attempt: å½“å‰å°è¯•æ¬¡æ•°

        Returns:
            æ¨¡å‹ç”Ÿæˆçš„æ–‡æœ¬
        """
        import time

        response_text = ""
        last_output_time = time.time()
        no_output_count = 0
        max_no_output_intervals = 3  # æœ€å¤šå®¹å¿3æ¬¡30ç§’æ— è¾“å‡º
        check_interval = 30  # æ¯30ç§’æ£€æŸ¥ä¸€æ¬¡æ˜¯å¦æœ‰æ–°è¾“å‡º

        # é‡å¤æ£€æµ‹ç›¸å…³
        repetition_check_interval = 500  # æ¯ç”Ÿæˆ500å­—ç¬¦æ£€æŸ¥ä¸€æ¬¡é‡å¤
        last_check_length = 0

        try:
            response = requests.post(
                self.api_endpoint,
                json=payload,
                stream=True,
                timeout=180  # è¿æ¥è¶…æ—¶3åˆ†é’Ÿ
            )
            response.raise_for_status()

            print(" [ç”Ÿæˆä¸­", end="", flush=True)

            for line in response.iter_lines():
                if line:
                    try:
                        data = json.loads(line)
                        if 'response' in data:
                            response_text += data['response']
                            last_output_time = time.time()
                            no_output_count = 0

                            # æ¯1000ä¸ªå­—ç¬¦æ˜¾ç¤ºä¸€ä¸ªç‚¹
                            if len(response_text) % 1000 < 50:
                                print(".", end="", flush=True)

                        # æ£€æŸ¥æ˜¯å¦å®Œæˆ
                        if data.get('done', False):
                            print("] ", end="", flush=True)
                            break

                        # å®šæœŸæ£€æŸ¥æ˜¯å¦æœ‰æ–°è¾“å‡º
                        current_time = time.time()
                        if current_time - last_output_time > check_interval:
                            no_output_count += 1
                            print(f"[{no_output_count}Ã—æ— è¾“å‡º]", end="", flush=True)

                            if no_output_count >= max_no_output_intervals:
                                print(f"\n  âš ï¸ æ£€æµ‹åˆ°æ¨¡å‹åœæ»ï¼ˆ{max_no_output_intervals * check_interval}ç§’æ— è¾“å‡ºï¼‰")
                                print(f"  ğŸ“Š å·²ç”Ÿæˆ {len(response_text)} å­—ç¬¦ï¼Œå¼ºåˆ¶åœæ­¢")
                                break

                            last_output_time = current_time

                        # æ™ºèƒ½é‡å¤æ£€æµ‹ï¼šæ¯ç”Ÿæˆä¸€å®šå­—ç¬¦åæ£€æŸ¥
                        if len(response_text) - last_check_length >= repetition_check_interval:
                            if self.detect_repetition(response_text):
                                print(f"\n  ğŸ”„ æ£€æµ‹åˆ°å†…å®¹é‡å¤ï¼Œè‡ªåŠ¨åœæ­¢")
                                print(f"  ğŸ“Š å·²ç”Ÿæˆ {len(response_text)} å­—ç¬¦")
                                break
                            last_check_length = len(response_text)

                    except json.JSONDecodeError:
                        continue

            # æ¸…ç†è¾“å‡º
            response_text = response_text.strip()
            if not response_text and attempt < 2:
                print(f"\n  âš ï¸ ç¬¬{attempt + 1}æ¬¡æµå¼è¯·æ±‚è¿”å›ç©ºç»“æœï¼Œé‡è¯•ä¸­...")
                return ""
            elif not response_text:
                print(f"\n  âŒ å¤šæ¬¡é‡è¯•ä»è¿”å›ç©ºç»“æœ")
                return ""

            return response_text

        except requests.exceptions.Timeout:
            print(f"\n  â±ï¸ æµå¼è¯·æ±‚è¶…æ—¶")
            return ""
        except Exception as e:
            print(f"\n  âŒ æµå¼å¤„ç†å‡ºé”™: {e}")
            return ""

    def process_transcript(self, transcript: str) -> str:
        """
        å¤„ç†ä¼šè®®é€å­—ç¨¿ï¼šå»å£è¯­åŒ–

        Args:
            transcript: åŸå§‹ä¼šè®®é€å­—ç¨¿

        Returns:
            å¤„ç†åçš„ä¼šè®®çºªè¦
        """
        print(f"\n{'=' * 60}")
        print("æ­¥éª¤1: ç²¾ç®€ä¹¦é¢åŒ–å¤„ç†")
        print(f"{'=' * 60}")

        # åˆ†å‰²æ–‡æœ¬ï¼ˆä½¿ç”¨1000å­—ç¬¦ç‰‡æ®µï¼Œæ–¹ä¾¿ç²¾ç®€ï¼‰
        chunks = self.split_text(transcript, max_chars=1000)
        print(f"æ–‡æœ¬å·²åˆ†å‰²æˆ {len(chunks)} ä¸ªç‰‡æ®µ (æ¯æ®µçº¦1000å­—ç¬¦)")
        print(f"é¢„æœŸè¾“å‡ºç¯‡å¹…: {int(len(transcript) * 0.8)}-{int(len(transcript) * 0.9)} å­—ç¬¦ (åŸæ–‡80%-90%)")

        processed_chunks = []
        total_output_length = 0
        dedup_count = 0

        for i, chunk in enumerate(chunks, 1):
            chunk_length = len(chunk)
            target_length_min = int(chunk_length * 0.8)
            target_length_max = int(chunk_length * 0.9)

            print(f"\n[{i}/{len(chunks)}] å¤„ç†ä¸­... (è¾“å…¥: {chunk_length} å­—ç¬¦, ç›®æ ‡: {target_length_min}-{target_length_max} å­—ç¬¦)", end=" ")

            # æ„å»ºæç¤ºè¯ï¼ŒåŒ…å«é•¿åº¦ä¿¡æ¯
            prompt = self.processing_prompt.format(
                text=chunk,
                text_length=chunk_length,
                target_length=target_length_min
            )

            # è°ƒç”¨æ¨¡å‹
            result = self.call_ollama(prompt)

            if result:
                # å…ˆå»é‡ï¼Œå†æ£€æŸ¥é•¿åº¦
                before_dedup = result
                result = self.remove_duplicates(result)

                # è®°å½•æ˜¯å¦è¿›è¡Œäº†å»é‡
                if len(result) < len(before_dedup):
                    dedup_count += 1
                    removed = len(before_dedup) - len(result)
                    print(f"\n  ğŸ”„ å»é‡: åˆ é™¤ {removed} å­—ç¬¦", end=" ")

                # æ£€æŸ¥è¾“å‡ºé•¿åº¦
                result_ratio = len(result) / chunk_length * 100

                # å¦‚æœè¾“å‡ºè¿‡çŸ­ï¼ˆ<60%ï¼‰ï¼Œå¯èƒ½ä¿¡æ¯ä¸¢å¤±ï¼Œå°è¯•é‡æ–°ç”Ÿæˆ
                if result_ratio < 60:
                    print(f"\n  âš ï¸ è¾“å‡ºè¿‡çŸ­ ({len(result)} å­—ç¬¦, {result_ratio:.1f}%)ï¼Œå¯èƒ½ä¿¡æ¯ä¸¢å¤±ï¼Œé‡æ–°ç”Ÿæˆ...")
                    result = self.call_ollama(prompt)
                    if result:
                        result = self.remove_duplicates(result)
                    result_ratio = len(result) / chunk_length * 100 if result else 0

                # æ ¼å¼åŒ–è¯´è¯äººæ®µè½ï¼ˆç¡®ä¿æ¯ä¸ªè¯´è¯äººå ä¸€è¡Œï¼‰
                result = self.format_speaker_paragraphs(result)

                processed_chunks.append(result)
                total_output_length += len(result)

                if result:
                    print(f"âœ“ è¾“å‡º: {len(result)} å­—ç¬¦ ({result_ratio:.1f}%)")

                    # è¯„ä»·æ¯”ä¾‹
                    if 80 <= result_ratio <= 90:
                        print(f"    âœ“ ç†æƒ³æ¯”ä¾‹")
                    elif result_ratio < 70:
                        print(f"    âš ï¸ è­¦å‘Š: ä»æœªè¾¾åˆ°ç›®æ ‡æ¯”ä¾‹ (ç›®æ ‡: 80%-90%)")
                    elif result_ratio > 100:
                        print(f"    âš ï¸ è­¦å‘Š: è¾“å‡ºåå¤š (å·²å»é‡)")
                    else:
                        print(f"    âœ“ å¯æ¥å—æ¯”ä¾‹")
                else:
                    print(f"âœ— é‡è¯•å¤±è´¥ï¼Œä½¿ç”¨åŸæ–‡")
                    processed_chunks.pop()
                    processed_chunks.append(chunk)
                    total_output_length += len(chunk)
            else:
                print(f"âœ— å¤„ç†å¤±è´¥")
                processed_chunks.append(chunk)  # å¤±è´¥æ—¶ä½¿ç”¨åŸæ–‡
                total_output_length += len(chunk)

        # åˆå¹¶æ‰€æœ‰å¤„ç†åçš„ç‰‡æ®µ
        processed_text = "\n\n".join(processed_chunks)

        # è¾“å‡ºç»Ÿè®¡
        overall_ratio = total_output_length / len(transcript) * 100
        print(f"\n{'=' * 60}")
        print(f"ç²¾ç®€ä¹¦é¢åŒ–å®Œæˆç»Ÿè®¡:")
        print(f"  åŸæ–‡æ€»é•¿: {len(transcript)} å­—ç¬¦")
        print(f"  è¾“å‡ºæ€»é•¿: {total_output_length} å­—ç¬¦ ({overall_ratio:.1f}%)")
        print(f"  ç›®æ ‡æ¯”ä¾‹: 80%-90%")
        print(f"  å»é‡æ¬¡æ•°: {dedup_count} ä¸ªsegments")

        if overall_ratio < 70:
            print(f"  âš ï¸ æ³¨æ„: è¾“å‡ºåå°‘ï¼Œå¯èƒ½ä¿¡æ¯ä¸¢å¤±")
        elif overall_ratio > 100:
            print(f"  âš ï¸ æ³¨æ„: è¾“å‡ºåå¤šï¼Œä¸å¤Ÿç²¾ç®€")
        elif 80 <= overall_ratio <= 90:
            print(f"  âœ“ è¾¾åˆ°ç†æƒ³æ¯”ä¾‹")
        else:
            print(f"  âœ“ åŸºæœ¬è¾¾æ ‡")
        print(f"{'=' * 60}")

        return processed_text

    def _generate_timestamped_filename(self, base_name: str) -> str:
        """
        ç”Ÿæˆå¸¦æ—¶é—´æˆ³çš„æ–‡ä»¶å

        Args:
            base_name: åŸºç¡€æ–‡ä»¶åï¼ˆå¦‚ "processed_chinese.txt"ï¼‰

        Returns:
            å¸¦æ—¶é—´æˆ³çš„æ–‡ä»¶åï¼ˆå¦‚ "processed_chinese_20251225_143020.txt"ï¼‰
        """
        from datetime import datetime

        # è·å–å½“å‰æ—¶é—´æˆ³
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

        # åˆ†ç¦»æ–‡ä»¶åå’Œæ‰©å±•å
        if '.' in base_name:
            name, ext = base_name.rsplit('.', 1)
            return f"{name}_{timestamp}.{ext}"
        else:
            return f"{base_name}_{timestamp}"


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # åˆå§‹åŒ–å¤„ç†å™¨
    formatter = ChineseFormatter(
        ollama_url="http://localhost:11434",
        model_name="yasserrmd/Qwen2.5-7B-Instruct-1M:latest"
    )

    # è¯»å–è¾“å…¥æ–‡ä»¶
    print("è¯»å–ä¼šè®®é€å­—ç¨¿...")
    try:
        with open("input_scripts/meeting_transcript.txt", "r", encoding="utf-8") as f:
            transcript = f.read()

        print(f"åŸå§‹æ–‡æœ¬é•¿åº¦: {len(transcript)} å­—ç¬¦")

        # å¤„ç†æ–‡æœ¬
        processed_chinese = formatter.process_transcript(transcript)

        # ç”Ÿæˆå¸¦æ—¶é—´æˆ³çš„æ–‡ä»¶å
        chinese_filename = formatter._generate_timestamped_filename("processed_chinese.txt")
        chinese_filepath = f"processed/{chinese_filename}"

        # ç¡®ä¿processedæ–‡ä»¶å¤¹å­˜åœ¨
        import os
        os.makedirs("processed", exist_ok=True)

        # ä¿å­˜ç»“æœ
        with open(chinese_filepath, "w", encoding="utf-8") as f:
            f.write(processed_chinese)

        # è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
        print(f"\n{'=' * 60}")
        print("ä¸­æ–‡ä¹¦é¢åŒ–å®Œæˆï¼ç»Ÿè®¡ä¿¡æ¯ï¼š")
        print(f"{'=' * 60}")
        print(f"åŸå§‹æ–‡æœ¬: {len(transcript)} å­—ç¬¦")
        print(f"å¤„ç†åä¸­æ–‡: {len(processed_chinese)} å­—ç¬¦")
        print(f"\nè¾“å‡ºæ–‡ä»¶å·²ä¿å­˜åˆ°:")
        print(f"  {chinese_filepath}")
        print(f"\næç¤º: å¯ä»¥è¿è¡Œ python english_translator.py æ¥è¿›è¡Œè‹±æ–‡ç¿»è¯‘")

    except FileNotFoundError:
        print("é”™è¯¯: æ‰¾ä¸åˆ° input_scripts/meeting_transcript.txt æ–‡ä»¶")
        print("\nè¯·ç¡®ä¿ input_scripts/meeting_transcript.txt æ–‡ä»¶å­˜åœ¨")
        print("æˆ–è€…ä½¿ç”¨ä»¥ä¸‹ä»£ç ç›´æ¥å¤„ç†æ–‡æœ¬ï¼š\n")
        print('transcript = """[è¯´è¯äºº1]ï¼šé‚£ä¸ª...æˆ‘è§‰å¾—...å‘ƒ...è¿™ä¸ªé¡¹ç›®..."""')
        print("results = formatter.process_transcript(transcript)")

    except Exception as e:
        print(f"å¤„ç†å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
